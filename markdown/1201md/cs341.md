title: CS 341 - Algorithms
---
# Lec 4
## Mergesort
mergesort: n integers, arbitrary order, sorting problem. Three steps: divide conquer and combine.

Do you have to divide evenly? No, but it will affect the run time.

If not power of two, need to use floor, ceil. Solve this problem recursively. Assume recursive call works,
then we can put the subsolution together. Then we have two sorted arrays, how can we combine it somehow to merge?

Take these two halves, then use `merge`. It's like using a zipper. The complexity ends in nlogn.

### Divide
call mergesort recursively on left and right. Then we have recursion tree.

### Conquer and combine
From the bottom, we just combine two single elements together, which is easy. As we keep going up, we need to merge these things.
Apply merge steps as we go up.

### Merge Steps
Maintain two indices of left index and right index. We compare two values in two indices.
The first element should be the smallest element, which is smaller of two smallest elements. Then increase the index of the left/right array.

### Pseudocode
See slides. The base case is if the array size is 1, then no need to sort. Divide the array by figuring out how many elements
should be in the left/right array. Make recursive call.

Merge code is a bit messy. Take two arrays, create a big array to hold all these elements. Three cases. Fairly straight forward code...

### Analysis
Now it's like loop analysis. Two steps for this: base case constant time. Here we need to copy elements, which is linear time.
Practically, we can just pass the indices without copying elements. Merge in the end takes linear time too. Not too hard to see why.
In `merge`, you can run nL + nR iterations before terminate. Certainly it's linear. But, we need to worry about recursive calls, deeper level
of recursion.

How do we analyze recursive structures?

We use recurrence relations. Represent complexity of problem by complexity of subproblems.

## Recurrence relation
infinite sequence of real numbers. You sometimes need initial values.

### Express the complexity of Mergesort with recurrence
divide takes linear time. Conquering takes T(ceil n/2) + T(floor n/2). combine linear.

When n=1, then it's easy. When n>1, then we use the expression in slide (just add expressions above together).

T(n) is a function, which is a recurrence relation. How can we solve for T(n)?

To make it easier, let \\(n=2^k\\).

It's not so useful if we have T(n), T(n-1), ...

### Recursion tree method
top call does \\(cn\\). Second level: Each mergesort call does \\(2(cn/2)=cn\\). The bottom level does \\(n(c)=cn\\).
Now the question is how much in total? If we sum up them all, we have \\(\\log n\\) levels. Thus total is \\(cn\\log n\\), which is
\\(O(n\\log n)\\).

Or you could do this with table, more analytically. Fill it in and sum them up in total runtime column.

#### Recursion tree method formalized
When n is power of 2, we can solve the problem easily.
- Start with one node tree.
- Grow two children.
- Repeat this process recursively, until you hit the base case.
- Sum up the costs over all nodes.

### Guess-and-check method
First substitute T(n-1) with T(n-2)+... (by defn). We unroll this recurrence by one level.
Simplify it a little bit. We preserve the patterns when simplifying. If haven't see the pattern yet,
unroll by one further level.

Identify the patern and guess what happens in the limit. Then we get

\\[T(n)=\\ldots=guess(n)\\]

Plug the initial value then get a closed form.

This is an **educated guess**. Here is enough for this course. To prove it, prove by induction (base case and induction step).

### Another Approach
with some experience, you might guess it's quadratic. You should have the form \\(an^2+bn+c\\) for some unknown constants a,b,c.
So just carry the unknown constants into the proof!

We set T(n+1) = guess(n+1), equalize the coefficients.

*Time Permitting* might not get here...

## Master Theorem for Recurrences
We start with a simplified version. \\(a\\ge 1, b> 1\\). Consider the recurrence
\\[T(n)=aT\\left({n\\over b}\\right) + \\Theta(n^y)\\]
where n is a power of b.

Denote \\(x=\\log_b a\\). Then
\\[
T(n)\\in
\\begin{cases}
\\Theta(n^x) \& if~y<x \\\\
\\Theta(n^x\\log n) \& if ~y=x \\\\
\\Theta(n^y) \& if~y>x. \\\\
\\end{cases}
\\]

You can prove it by kinda recursion tree methods.
