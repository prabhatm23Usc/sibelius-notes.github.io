# Lec 4
## Mergesort
mergesort: n integers, arbitrary order, sorting problem. Three steps: divide conquer and combine.

Do you have to divide evenly? No, but it will affect the run time.

If not power of two, need to use floor, ceil. Solve this problem recursively. Assume recursive call works,
then we can put the subsolution together. Then we have two sorted arrays, how can we combine it somehow to merge?

Take these two halves, then use `merge`. It's like using a zipper. The complexity ends in nlogn.

### Divide
call mergesort recursively on left and right. Then we have recursion tree.

### Conquer and combine
From the bottom, we just combine two single elements together, which is easy. As we keep going up, we need to merge these things.
Apply merge steps as we go up.

### Merge Steps
Maintain two indices of left index and right index. We compare two values in two indices.
The first element should be the smallest element, which is smaller of two smallest elements. Then increase the index of the left/right array.

### Pseudocode
See slides. The base case is if the array size is 1, then no need to sort. Divide the array by figuring out how many elements
should be in the left/right array. Make recursive call.

Merge code is a bit messy. Take two arrays, create a big array to hold all these elements. Three cases. Fairly straight forward code...

### Analysis
Now it's like loop analysis. Two steps for this: base case constant time. Here we need to copy elements, which is linear time.
Practically, we can just pass the indices without copying elements. Merge in the end takes linear time too. Not too hard to see why.
In `merge`, you can run nL + nR iterations before terminate. Certainly it's linear. But, we need to worry about recursive calls, deeper level
of recursion.

How do we analyze recursive structures?

We use recurrence relations. Represent complexity of problem by complexity of subproblems.

## Recurrence relation
infinite sequence of real numbers. You sometimes need initial values.

### Express the complexity of Mergesort with recurrence
divide takes linear time. Conquering takes T(ceil n/2) + T(floor n/2). combine linear.

When n=1, then it's easy. When n>1, then we use the expression in slide (just add expressions above together).

T(n) is a function, which is a recurrence relation. How can we solve for T(n)?

To make it easier, let n=2^k.

It's not so useful if we have T(n), T(n-1), ...

### Recursion tree method
top call does \\(cn\\). Second level: Each mergesort call does \\(2(cn/2)=cn\\). The bottom level does \\(n(c)=cn\\).
Now the question is how much in total? If we sum up them all, we have \\(\\log n\\) levels. Thus total is \\(cn\\log n\\), which is
\\(O(n\\log n)\\).
