title: CS 350 - Operating Systems
---
miscellaneous notes... Also, check my [help page](../cs350help/)

Lesley's lectures recording starts from lec 7.
# Intro
OS:
- manages resource
- creates execution environments.

Three views:
1. Application view: provide services
    - part cop: provides protection from program errors. part facilitator: abstract interface to underlying system
    - What services?
        - resources
        - interfaces
        - isolates running programs
2. System view: solve problems
    - manages the hardware resources
    - allocates resources
    - controls the access to or sharing of resources among programs.
3. Implementation view: how is it built?
    - **concurrent**: multi progs or seqs of instrs *running, or appearing to run, at the same time*.
    - **real-time**: must respond to an event in a specific amount of time.

**kernel**: The operating system kernel is the part of the operating system that responds to system calls, interrupts and exceptions.

**operation system**: The operating system as a whole includes the kernel, and may include other related programs that provide servicesfor applications.  This may include things like:
- utility programs (e.g. disk defragmenter, task manager)
- command interpreters (e.g. cmd.exe in Windows, bash in Linux)
- programming libraries (e.g. POSIX threads in Linux)

**monolithic kernel**: the entire operating system (which includes the device drivers, file system, and the application IPC) is working in kernel space. "everything and the kitchen sink" is a part of the
kernel. This includes device drivers, file system, virtual memory, etc.

**microkernel**: only absolutely necessary components are a part of the
kernel. All other elements are user programs.

**real-time OS**: an OS with stringent event response times,
guarantees, and preemptive scheduling.

The **execution environment** provided by the OS includes a variety of
**abstract entities** that can be manipulated by a running program.

# Threads
seq of instructions.

a single program can have
- diff threads responsible for different roles
- multi threads --- same roles

Reasons we use threads:
- Efficient Use of Resource
- parallelism
- responsiveness
- priority
- modularization

A thread **blocks**, when it ceases execution for a period of time, or, until some condition has been met (e.g.  a website respondsor a user moves the mouse). CPU is idle.

Key ideas:
- A thread can create new threads using `thread_fork`
- new threads start execution in a function specified as a param to `thread_fork`
- original (called `thread_fork`) and new thread proceed concurrently
- All threads share access to program's global vars and heap
- each thread has its own stack (private to that thread)

In the OS, a thread is represented as a struct or object

An example of using `thread_fork` to create a new thread that is running the function `vehicle_sumulation`.

```c++
for (i = 0; i < NumThreads; i++) {
    error = thread_fork("vehicle simulation thread", NULL,
        vehicle_simulation, NULL, i);
    if (error) {
        panic("traffic sim: thread_fork failed: %s",
            strerror(error));
    }
}
```

**Interface**
- create a new thread:
```c++
int thread_fork(
    const char *name, // name of new thread
    struct proc *proc, // thread’s process
    void (*func) // new thread’s function
        (void *, unsigned long),
    void *data1, // function’s first param
    unsigned long data2 // function’s second param
);
```
- terminate the calling thread: `void thread_exit(void);`
- volutarily yield execution: `void thread_yield(void);`

<div class='ex'>
<div class="ex-title">
In-Class Problems: forkprint
</div>

<div class='ex-content'>
<pre>
<code>
main() {
    helper(NULL,0);
}

void
helper(void \*p, unsigned long i) { /\* parameter p is not used \*/
    if (i < 3) {
        kprintf("%ld",i);           /\* print the value of i \*/
        thread_fork("helper1",NULL,helper,NULL,i+1); /\* fork thread to run helper(NULL,i+1) \*/
        thread_fork("helper2",NULL,helper,NULL,i+1); /\* fork thread to run helper(NULL,i+1) \*/
    }
    if (i==0) {
        kprintf("%ld",i);
    }
    thread_exit();
}
</code>
</pre>
<p>Which of the following outputs could possibly be generated by this program?</p>
<ol type="a">
    <li> 01221220</li>
    <li> 01120222</li>
    <li> 01342560</li>
    <li> 01222120</li>
    <li> 01122220</li>
    <li> 01234560</li>
    <li> 01212220</li>
    <li> 00112222</li>
</ol>
<p>**answer**: cdf not possible</p>
<p>Explanation (credit to Nathan Lo on Piazza): We can eliminate c and f as we only print \\(i\\) when \\(i < 3\\), so we should never print anything \\(>= 3\\). </p>

</p>d is trickier: it's impossible because we observe we print before we fork, so it's impossible to have a single 1, followed by 3 2's, as this implies we had one thread with \\(i=1\\) fork into 3 threads with \\(i=2\\). Notice that each thread with \\(i=1\\) spawns exactly 2 threads, so this is impossible.</p>

<p>
The others are correct: the way to see this which was discussed in class was to draw out the "call tree" (fork tree?) of threads and note exactly when the print statement is with respect to the forks.</p>
</div>
</div>

## Review from whatever course
**Review: Sequential Program Execution**
- fetch instr (from code segment) that the Program counter points to
- decode and execute the instr and
- increment the PC

> Note: In CS241 we names the MIPS registers $0, $1,
$2, etc. In CS350 we referred to them by their default function,
e.g. v0, a0, t0.

`t0-t7`: temps (caller-save). if the calling subroutine has a value in one of
these registers that it wants preserved, it should store the value
on the stack before calling any subroutines and restore it
afterwards.

`s0-s7`: saved (callee-save). if the called subroutine uses one of these
registers is must first store its current value on the stack and
restore it before exiting

This strategy tries to *miminize situations where the callee is saving values that the caller is not using*.

**Review: The Stack**: grows downward in slides.

Conceptually, each thread executes sequentially using its private register contents and stack.

## Concurrent Program Execution
**Key Question**: What is shared between threads?
- They are both executing (perhaps at the same time on different cores) the same program.
- Both threads share the same code, read-only data, global variables and heap.
- Each thread has its own stack and PC.

Three options:  
- Hardware support (P processors, C cores, M multithreading per core)
- Timesharing: Multiple threads tak turns on the same hardware; rapidly switching.
- Hardware support + Timesharing

When **timesharing**, the switch from one thread to another is called a **context switch**.

## Context Switch
- decide which thread will run next (scheduling)
- save register contents of current thread
- load register contents of next thread

Thread context (register values) must be saved/restored
carefully, since thread execution continuously changes the
context.

The threads share the CPU, giving the user the illusion of multiple
programs running at the same time.

**On MIPS**

```assembly
switchframe_switch:
    /* a0: address of switchframe pointer of old thread. */
    /* a1: address of switchframe pointer of new thread. */

    /* Allocate stack space for saving 10 registers. 10*4 = 40 */
    addi sp, sp, -40
    sw ra, 36(sp)   /* Save the registers */
    sw gp, 32(sp)
    sw s8, 28(sp)   /* a.k.a. frame pointer */
    sw s6, 24(sp)
    sw s5, 20(sp)
    sw s4, 16(sp)
    sw s3, 12(sp)
    sw s2, 8(sp)
    sw s1, 4(sp)
    sw s0, 0(sp)

    /* Store the old stack pointer in the old thread */
    sw sp, 0(a0)

    /* Get the new stack pointer from the new thread */
    lw sp, 0(a1)
    nop             /* delay slot for load */

    /* Now, restore the registers */
    lw s0, 0(sp)
    lw s1, 4(sp)
    lw s2, 8(sp)
    lw s3, 12(sp)
    lw s4, 16(sp)
    lw s5, 20(sp)
    lw s6, 24(sp)
    lw s8, 28(sp)   /* a.k.a. frame pointer */
    lw gp, 32(sp)
    lw ra, 36(sp)
    nop             /* delay slot for load */

    /* and return. */
    j ra
    addi sp, sp, 40 /* in delay slot */
    .end switchframe_switch
```

C function `thread_switch` calls the assembly language subroutine `switchframe_switch`.

`thread_switch` caller: save/restore the *caller-save* regs, including return address (ra)

`switchframe_switch` callee: save/restore *callee-save* regs. In OS/161 the frame pointer is callee saved.

`switchframe_switch`, saves callee-save registers to the old
thread’s stack; it restores the callee-save registers from the new
thread’s stack.

MIPS R3000 is pipelined; delay-slots are used to protect against *load-use hazards*, *control hazards*.

*What Causes Context Switches?*
1. `thread_yield`: voluntarily allows other threads to run
2. `thread_exit`: terminated
3. blocks, via a call to `wchan_sleep`: it is waiting for some resource (such as network access) or for some event to happen
4. is preempted: it involuntarily stops running (because the thread scheduler stopped it)

## Thread States
<img src="/pics/thread_state.png" width=100%>

Running: current executing. Ready: ready to execute. Blocked: waiting for sth, so not ready to execute.

`thread_yield` (C function) yield CPU. `thread_yield` calls (C function) `thread_switch` (high level context switch). `thread_switch` chooses a new thread then calls (assembly subroutine) `switchframe_switch` (low level).

**Scheduling quantum** imposes a limit on processor time: upper bound on how long a thread can run before it must yield the CPU.

An **interrupt** is an event that occurs during the execution of a program.
- caused by system devices (hardware): timer, disk controller, network interface card.
- when interrupt occurs,  the hardware automatically transfers control to a fixed location in memory (specified by the designer of the CPU).
- At that location, the the thread library must place a procedure called an **interrupt handler**.
- the interrupt handler normally:
    - creates a trap frame to record the thread context at the time of the interrupt,
    - determines which device caused the interrupt and performs device-specific processing,
    - restores the saved thread context from the trap frame and resumes execution of the thread.

**Thread context** is all the information (i.e. register values) needed to resume executing a thread after it has been suspended.
and it's stored in a switchframe.

When a thread is interrupted all the register values are stored in a **trap frame**.

## Preemptive Scheduling
- Threads may block or yield before their quantum has expired.
- If a thread has run too long, the timer interrupt handler preempts the thread by calling `thread_yield`.
- The preempted thread changes state from running to ready, and it is placed on the **ready queue**.
- OS161 threads use preemptive round-robin scheduling:
    - scheduler maintains a queue of threads, often called the ready queue.
    - On a context switch, the running thread is moved to the end of the ready queue, and the first thread on the queue is allowed to run.
    - Newly created threads are placed at the end of the ready queue.
-  Threads can be migrated to other processors or interrupted by device so the order they run is nondeterministic.

Then see two examples in slides (one and two threads): involuntary/voluntary context switch.

<div class='ex'>
<div class="ex-title">
Review Questions
</div>

<div class='ex-content'>
<p>Every thread has its own stack.</p>
<p>Threads can only run in parallel (execute instructions simultaneously) if there is hardware support (via multiple CPUs, cores, etc.).</p>
<p>Which of the following are NOT reasons to use threads?</p>
<ul>
    <li>resource utilization</li>
    <li>*isolation from errors*</li>
    <li>computation time</li>
    <li>organization</li>
</ul>
</div>
</div>



<div class='ex'>
    <div class="ex-title">In-Class Problems: twothreads</div>
    <div class='ex-content'>
    Suppose that there are two threads in a system that uses preemptive round-robin scheduling with a
    scheduling quantum of Q milliseconds. The system has a single processor. Each thread runs a function
    which behaves as follows

<pre>
<code>
for i from 1 to N do
    compute for C milliseconds
    sleep for S milliseconds
end
</code>
</pre>

    At the end of its for loop, a thread is finished and it exits. During the “compute” part of each iteration,
    a thread is runnable (running or ready to run). During the “sleep” part of each of its iterations, a thread is
    blocked. For both parts of this question assume that C < Q and C < S.
<br><br>
    **a.** First, assume that C < S and C < Q. Suppose that both of the threads are created at time t = 0. At what time will both of the threads be finished? Answer in terms of Q, N, C, and S, as necessary.
<br><br>
    **Soln**: Let `C -> XXX, S -> YYYY.`

<pre>
<code>
T_0     XXX | YYYYXXX | ... |
T_1         | XXXYYYY | ... | YYYY
</code>
</pre>

    Thus the answer is \\(N(C+S)+C\\)
<br><br>
    **b.** Answer the same question, but this time assume that S < C < Q.
<br><br>
    **Soln**: Let `C -> XXXX, S -> YYYY.`

<pre>
<code>
T_0     XXXX | YYYZXXXX | ... | ... YYYZ |
T_1          | XXXXYYYZ | ... | ... XXXX | YYY
</code>
</pre>

    Thus the answer is \\(2NC + S\\)
    </div>
</div>




# Synchronization
Concurrent threads interact with each other in a variety of ways:
- All threads in a concurrent program **share access** to the program’s global variables and the heap.
- Threads share access, through the operating system, to system devices, such as the hard drive, the display, etc.

A common synchronization problem is to enforce **mutual exclusion**.

The part of a concurrent program in which a shared object is accessed (e.g. the intersection) is called a **critical section**.

This coordination of access to a shared resource is called **synchronization**.

*What happens if several threads try to access the same global variable or heap object at the same time?*

We require that concurrent programs will run correctly regardless of which order the threads are executed.

A **race condition** is when the program result depends on the order of execution.

To find the critical sections...
- Inspect each variable and ask is it possible for multiple threads to read and write it concurrently?
- Constants and memory that all threads only read do not cause race conditions.

The course text defines a critical section as
> A critical section is a piece of code that accesses a shared
variable (or more generally, a shared resource) and must not be
concurrently executed by more than one thread.

```c++
Acquire(bool *lock) {
while (*lock == true) {}; // spin until lock is free
    *lock = true;       // grab the lock
}

Release(book *lock) {
    *lock = false;      // give up the lock
}
```
This approach fails because between the time a thread breaks out of
the while loop (i.e. `*lock = false`) and sets `*lock` to `true` it could
be preempted and another thread could come along and set `*lock` to
`true`.

We want to create a function so that between when we test `*lock` and
we set it we do not want another thread to change its value.

We will call this approach **test-and-set** and we will look at three
approaches to implementing it.
1. x86-64’s `xchg`. Atomic

```c++
Acquire(bool *lock) {
    while (xchg(lock,true) == true) {}; // test and set
}

Release(bool *lock) {
    *lock = false;                      // give up lock
}
```

This construct is known as a **spinlock**, since a thread busy-waits (loops or spins) in `Acquire` until the lock is free.
2. ARM’s `LDREX` and `STREX`
3. MIPS’s `ll` and `sc`

More on MIPS Synchronization Instructions
- `ll` (load linked): load value at address addr.
- `sc` (store conditional): store new value at addr if the value at addr has not changed since the instruction `ll` was executed.
- `sc` returns SUCCESS if the value stored at the address has not changed since `ll` was executed.
- `sc` does not check what that value at the address is. It only checks if it has changed.

```c
MIPSTestAndSet(addr, new_val) {
    old val = ll addr           // test
    status = sc addr, new_val   // set
    if ( status == SUCCEED ) return old_val
    else return true            // lock is being held
}

Acquire(bool *lock) {           // spin until hold lock
    while( MIPSTestAndSet(lock, true) == true ) {};
}
```

|     | lock value at `ll` | lock value before `sc` | lock value after `sc` | status | Lock State |
| :-- | :--: | :--: | :--: | :--: | :--|
| 1. | false |  false | true | succeed| **lock acquired** |
| 2. |true| true| true| succeed| **keep spinning, no lock** |
| 3. |false| true| true| fail| **keep spinning, no lock** |
| 4. |true| false| false| fail| **keep spinning, no lock** |




## Spinlocks in OS/161
A **spinlock** is a lock that spins, i.e. it repeatedly tests the lock’s availability in a loop until the lock is obtained.

When threads uses a spinlock they **busy-wait** i.e. it uses the CPU while they wait for the lock.

```c++
struct spinlock {
    volatile spinlock_data_t lk_lock;
    struct cpu *lk_holder;
};

void spinlock_init(struct spinlock *lk);
void spinlock_acquire(struct spinlock *lk);
void spinlock_release(struct spinlock *lk);
```

`spinlock_acquire` calls `spinlock_data_testandset` in a loop until the lock is acquired.

OS/161 uses `ll` and `sc` to implement test-and-set.

## OS/161 Locks
In addition to spinlocks, OS/161 also has **locks**.

Like spinlocks, locks are used to enforce mutual exclusion, i.e. they
are a type of mutex.

```c++
struct lock *mylock = lock_create("LockName");

lock_acquire(mylock);
    // critical section
lock_release(mylock);
```

Spinlocks spin whereas locks block:
- `spinlock_acquire` spins until the lock can be acq
- `lock_acquire` blcoks until the lock can be acq

Locks can be used to protect larger critical sections without being a burden on the CPU.

## Spinlocks and Locks: Similarities and Difference
For both you would
- acquire it, access the critical section, then
- release it when you have finished accessing the critical section.

Both have owners and can only be released by their owner.
- A spinlock is owned by a CPU.
- A lock is owned by a thread.

In OS/161, and most modern OSs, interrupts are disabled on the CPU that holds the spinlock. Preemption is disabled on that CPU (hence, owned by that CPU) but not other CPUs, which minimizes spinning. So don't use spinlocks to protect large critical sections.

Spin Locks
- `void spinlock_init(struct spinlock *lk)`
- `void spinlock_acquire(struct spinlock *lk)`
- `void spinlock_release(struct spinlock *lk)`
- `bool spinlock_do_i_hold(struct spinlock *lk)`
- `void spinlock_cleanup(struct spinlock *lk)`
Locks
- `struct lock *lock_create(const char *name)`
- `void lock_acquire(struct lock *lk)`
- `void lock_release(struct lock *lk)`
- `bool lock_do_i_hold(struct lock *lk)`
- `void lock_destroy(struct lock *lk)`

## Wait Channels
When a thread blocks, it stops running, i.e. stops using the processor.

```c++
void wchan_sleep(struct wchan *wc);     // blocks calling thread on wait channel wc

void wchan_wakeall(struct wchan *wc);   // unblock all threads sleeping on wait channel wc

void wchan_wakeone(struct wchan *wc);   // unblock one thread sleeping on wait channel wc

void wchan_lock(struct wchan *wc);      // prevent operations on wait channel wc
```

## Semaphore
is a synchronization primitive that can be used to enforce mutual exclusion requirements. It can also be used to solve other kinds of synchronization problems.

It's an object has an integer value which supports two operations:
- **P**: if the semaphore value is greater than 0, decrement it. Otherwise, wait until the value is greater than 0 and then decrement it.
- **V**: : increment the value of the semaphore

By defn, `P` and `V` operations of a semaphore are atomic.

**Difference between a lock and a semaphore**
- `V` does not have to follow `P`
- A semaphore can start with a count of 0
- Calling `V` increments the semaphore by 1.
- Semaphore do not have owners.

**Producer/Consumer Synchronization with Bounded Buffer**
1. producers: threads that (create and) add items to a buffer
2. consumers: thread that remove (and process) items from the buffer

## Semaphore Implementation
```c
struct semaphore {
    char *sem_name;             // for debug purposes
    struct wchan *sem_wchan;    // queue where threads wait
    struct spinlock sem_lock;   // to synch access to this struct
    volatile int sem_count ;    // value of the semaphore
};

void
P(struct semaphore *sem)
{
        KASSERT(sem != NULL);
        KASSERT(curthread->t_in_interrupt == false);

	spinlock_acquire(&sem->sem_lock);
        while (sem->sem_count == 0) {       // prepare to sleep
    		wchan_lock(sem->sem_wchan);
    		spinlock_release(&sem->sem_lock);
            wchan_sleep(sem->sem_wchan);    // sleep
    		spinlock_acquire(&sem->sem_lock);
        }
        KASSERT(sem->sem_count > 0);
        sem->sem_count--;                   // claim one item
	spinlock_release(&sem->sem_lock);
}

void
V(struct semaphore *sem)
{
        KASSERT(sem != NULL);

	spinlock_acquire(&sem->sem_lock);

        sem->sem_count++;
        KASSERT(sem->sem_count > 0);
	wchan_wakeone(sem->sem_wchan);

	spinlock_release(&sem->sem_lock);
}

```

Incorrect Semaphore Implementation: Suppose `spinlock_release` preceeds `wchan_lock`... Thread 1 is blocked on a semaphore that has resources.

Instead Suppose `wchan_lock` preceeds `spinlock_release`.

## Condition Variables
OS/161 supports another common synchronization primitive:
**condition variables**.

Each CV is intended to work together with a lock: only used from  within the critical section that is protected by the lock.

1. **wait**: This causes the calling thread to block, and it releases the lock associated with the condition variable. Once the thread is unblocked it reacquires the lock.
2. **signal**: If threads are blocked on the signaled condition variable, then one of those threads is unblocked.
3. **broadcast**: Like signal, but unblocks all threads that are blocked on the condition variable.

```c
// solution 1
int SafeToWalk() {
    lock_acquire(geeseMutex);
    while (numberOfGeese > 0) {
        lock_release(geeseMutex); // allow access
        // allow some context switch to accur and another thread might then acq the lock and modify numberOfGeese
        lock_acquire(geeseMutex); // restrict access
    }
    GoToClass();
    lock_release(geeseMutex);
}

// soln2
int volatile numberOfGeese = 100;
lock geeseMutex;
cv zeroGeese;
int SafeToWalk() {
    lock_acquire(geeseMutex);
    while (numberOfGeese > 0) {
        cv_wait(zeroGeese, geeseMutex);
    }
    GoToClass();
    lock_release(geeseMutex);
}
```
> Lesley's recordings start here.

We left last class: semaphores and CV. Different primitives for concurrency problem.
- Semaphore:
    - any number of resources. `P` acquire, `V` release. Use as a lock. Keep in mind it's not lock. There's no built-in protection for it, no track of ownership: someone else tries to release **binary** semaphore on me, they can, and we have race condition.
    - **counting**: keep counting # of resources
    - **barrier**: force one thread to wait for others to complete before we proceed. Not separate implementation.
    - keep in mind do not touch the counter directly. It's a shared resource.
- CV: you are inside critical section. You really need condition to be true to proceed. But the lock you own it the lock you are using to protect the shared variable that has unique value. CV lets us do simultaneously safely let go of the lock and fall asleep so that another thread can modify the condition and wake us back up.<br>
Most of the type we are using: when we are woken back up, we need to recheck the condition.

## Other sources of Race conditions
The previous RC is because of the code you wrote.

**memory models** describe how thread access to memory in shared regions behave. a memory model tells the compiler and CPU which optimizations can be performed

**compiler**: it does optimization. Reduce the # of assembly instrs: load and stores.
```c++
int sharedTotal = 0;
int FuncA() {
    //... code that uses sharedTotal ...
}
int FuncB() {
    //... code that uses sharedTotal ...
}
```
Instead of go to RAM, much more efficient to keep it in register: register allocation. Here's the problem: It doesn't know if A and B run at the same time. If one put in `$3` one in `$8`. Big race condtion. There are also others optimization.

Always load/store var from RAM, not reordering: `volatile`. Disable compiler optimizations, but not solve race conditions in your codes.

**CPU**: fetch the program counter, decode it, execute it and move on. But it's more complex: pipeline, branch prediction, reorder instrs... It also has memory model and so that you can notify the CPU: multi-threaded code is going on here. Barrier or fence instrs to entry gate: CPU stops rearranging. And until end gate, turn on opt back on. Don't need to worry in OS161.

<div class='ex'>
    <div class="ex-title">In class problem: semaphore to CV</div>
    <div class='ex-content'>
<table>
    <thead>
        <tr>
            <th>Global Vars</th>
            <th>Initialization</th>
            <th>Function `func1`</th>
            <th>Function `func2`</th>
        </tr>
    </thead>
<tbody>
<tr>
<td>
<pre>
struct semaphore \*sa;
struct semaphore \*sb;
struct semaphore \*sc;
</pre>
</td>
<td>
<pre>
sa = sem_create("A", 1);
sb = sem_create("B", 1);
sc = sem_create("C", 0);
</pre>
</td>
<td>
<pre>
void func1() {
    P(sa);
    funcA();
    V(sa);
    P(sc);
}
</pre>
</td>
<td>
<pre>
void func2() {
    P(sb);
    funcB();
    V(sb);
    P(sc);
}
</pre>
</td>
</tr>
</tbody>
</table>

Re-implement `func1` and `func2` using lock and cv.
<br><br>
**Solution**: func1 is consumer, func2 is producer.
<br>
<pre>
<code>
lock A, B, C;
CV cv;
count = 0;

func1 {
    lock_acq(A)
        funcA()
    lock_rel(A)

    lock_acq(C)
        count--
        if (count < 0) cv_wait(cv, C)
    lock_rel(C)
}

func2 {
    lock_acq(B)
        funcB()
    lock_rel(B)

    lock_acq(C)
        if (count < 0) cv_signal(cv, C)
        count++
    lock_rel(C)
}
</code>
</pre>
    </div>
</div>




## Deadlocks
```c++
lock lockA, lockB;

int FuncA() {
    lock_acquire(lockA)
    lock_acquire(lockB)
    ...
    lock_release(lockA)
    lock_release(lockB)
}

int FuncB() {
    lock_acquire(lockB)
    lock_acquire(lockA)
    ...
    lock_release(lockB)
    lock_release(lockA)
}
```

- Thread 1 executes `lock_acquire(lockA)`
- Thread 2 executes `lock_acquire(lockB)`
- Thread 1 executes `lock_acquire(lockB)`
- Thread 2 executes `lock_acquire(lockA)`

Deadlock only shows up under certain order of thread execution. It is very difficult to detect: between problematic wait and wait on purpose.

Generally, it happens when multiple threads, each acquire multiple resources.
1. don't try to aquire a lock if you have already owned.
2. Two strategies

**No hold and wait**: while you own the resources, no wait (no block, no spin).
<br> Being able to aquire all resources at once. We need special implementation of lock to support.

If you can't acq a lock, you can't fall asleep. Special implementation of acq.

```c++
// true if you get the lock
// false if cannot, but do not go to sleep.
bool try_acq(lk) {
    acq(lk->spin)
        if (lk->held) {
            release(lk->spin)
            return false
        }
        lk->held =  true
        lk->owner = me
    release(lk->spin)
    return true
}
```

Solution to previous deadlock:

```c++
acq(A) // I own nothing and safely acquire

while (!try_acq(B)) {
    release(A)
    // if you want to improve the performance, put yield here
    acq(A)
}
```

But it's nasty if we want acq lots of things, also it's spinning.

**Resource ordering**: assign every single resource a single number. Acquire them only in strictly increasing order. There's problem if different programmers have different rules.
<div class='ex'>
<div class="ex-title">
In class problem: queuexfer
</div>

<div class='ex-content'>
Suppose that a threaded program has N queues of items. The program needs to support an operation called `Transfer(i,j)`. Each call to `Transfer` will transfer a single item from the i-th queue to the jth queue,
unless there is nothing in the i-th queue, in which case the call will not affect the queues.
<br>The program will have multiple concurrent threads, each of which may call `Transfer` zero or more times.
<br>How would you use locks to ensure that `Transfer` operations are atomic? Specifically, how many locks
would you use, what would each lock protect, and when would the locks be acquired and released to ensure
that transfers are atomic?

<br><br>
One way: no hold and wait. Downside: cannot both lock at the same time, busy wait: spin.

<br><br>
Alternate: resource ordering.
<pre>
<code>
Transfer(Q_a, Q_b) {
    // check resource number
    if (a < b) {
        acq(qa)
        acq(qb)
    } else {
        acq(qb)
        acq(qa)
    }
}
</code>
</pre>
Downside: you need to sort if you acq all resources \\(\Theta(n\log n)\\).
</div>
</div>





# Process
is an is an environment in which an application program runs.

includes virtualized resources that its program can use: one (or more) threads

each program’s process *isolates* it from other programs in other processes

## fork
creates a brand new process (the child) that is a clone of the original (the parent)
- new process structure
- new address space
- new thread array
- new PID
clone
- content of address space

## _exit
terminate the calling process. Not necessarily disappear: stop executing, address space get cleaned, but if it has parents, that parent is alive. Parents want to figure out why child dies. Leave behind a msg for parent.

## waitpid
offer synchronization between processes. Cause the caller to wait for the PID process to terminate. Restricted to: parent can only wait for their children to die.

Children cannot wait for their parents. If they die, they are old.

## execv
Now, the last of the five system calls: execv. It does not create a new process.

It changes the program and a existing process is running. Process structure is same, but the address space has changed.

`Hello world` program, and call `execv matlab`.

Create a new address space, and load the program into the new address space.

Parent child relationship: If you go home tonight, and dye your hair in green, your parent may not like it. But they cannot change genetically they are your parents. The same is true for processes: you can change the program that you are running but you cannot change parent-child relationship.

Example from slides:
```c++
int main()
{
    int rc = 0;
    char *args[4];

    args[0] = (char *) "/testbin/argtest";
    args[1] = (char *) "first";
    args[2] = (char *) "second";
    args[3] = 0; // null terminator

    rc = execv("/testbin/argtest", args); // takes two parameters
    printf("If you see this execv failed\n");
    printf("rc = %d errno = %d\n", rc, errno);
    exit(0);
}
```

takes two parameters. Many times you need command line arguments. Array of pointers.

`execv` if it succeeds, does not return. Because it successfully created the new address space and new program. We successfully loaded the program into it. The calling program will not exist if `execv` succeeded. The only time it returns is when it fails. Why would it fail? 30mb ram then run Matlab. It's possible that there's no memory left or you call the program wrongly. When you write your `execv`, you need to test for these error cases and return appropriate error codes. (but cs350 stuff don't have such tests on these).

You notice that we can't tell how many arguments there are automatically. So we use null terminator array and you are responsible for counting.

Another example. This time with fork.

```c++
int main()
{
    char *args[4];
    /* set args here */
    rc = fork(); /* returns 0 to child, pid to parent */
    if (rc == 0) {
        status = execv("/testbin/argtest",args);
        printf("If you see this execv failed\n");
        printf("status = %d errno = %d\n", status, errno);
        exit(0);
    } else {
        child_pid = rc;
        parent_code();
        p = waitpid(child_pid,&child_exit,0);
    }
}
```

Now I have two processes. Even though they are separate, but their contents are identical with the exception of value of `rc`. 0 for child, pid for parent.

If child, `execv`. Child is running different process. Parent can still wait for child to terminate. Parent child relationship is not impacted by `execv` which stays forever.

Go home and write thread fork bomb.

# System Calls
- Process management calls, e.g., fork, are called by user programms. They are also system calls.
- **System calls** are the interface between user processes and the kernel.
- The kernel code runs at the highest privilege level, where any CPU instructions can be executed.
- Program in user space cannot execute code or instructions belonging to a higher-level of privilege.

## Kernel Privilege
The CPU implements different levels (or rings) of execution
privilege as a security and isolation mechanism. Kernel code at highest level.

**Key Question**: Since application programs cannot directly call the
kernel, how does a program make a system call such as `fork`?
1. Interrupts
    - Interrupts are raised by devices (hardware)
    - An interrupt causes the CPU to transfer control to a fixed location in memory (specified by the designers of the CPU) where an interrupt handler must be located.
    - Interrupt handlers are part of the kernel.
2. Exceptions
    - conditions that occur during the execution of a program instruction.
    - The CPU handles exceptions like it handles interrupts:
        - Control is transferred to a fixed location, where an exception handler is located.
        - The processor switches to privileged execution mode.
    - In OS/161 *the same routine is used to handle exceptions and interrupts*.

When CPU receives interrupt or exception, your program stops executing and exception/interrupt handler is called, part of kernel code. Then CPU switches from unprivileged mode to privileged mode, so now we can execute kernel's code. Produces the trapframe: storing every single reg value (including special ones) so we can return to the exact point of program execution after we handle.


Extra: [Spectre and Meltdown Papers](https://meltdownattack.com/).



**Key Question**: There is only one `syscall` exception, `EX_SYS`. So how does the OS distinguish between a `fork` and `getpid` system call?

*system call codes*
- The kernel expects the code to be placed in a specified location before executing `syscall` (for OS/161 on MIPS, reg `v0`)
- the codes and code location are part of the kernal ABI (Application Binary Interface)

ABI is not secret because we need user land to know that info and it absolutely tells nothing what's happening in the kernel -- blackbox.

Example:
- `li v0, 0` where 0 is the syscall code for `fork`.
- Syscall parameters: load into regs `a0` to `a3`. If more, put into stack or heap, and put the address here.
- Use `syscall` to raise exception.

Return two values:
- reg `a3`: success/fail
- reg `v0`: return value/error code

## User and Kernel Stacks
Every OS/161 process thread has **two** stacks, although it only uses one at a time.

The only thing that goes into user stack us the user application stack. But before we save the trapframe, we must change the stack pointer from user stack to kernel stack.
1. The User (Application) Stack is used while the thread is executing application code.
    - Saw this in [CS 241](/19-05/CS241/).
    - The kernel creates this stack when it sets up the virtual address space for the process (or thread if the OS supports multi-threaded code).
2. The Kernel Stack is used while the thread is executing kernel code, i.e. after an exception or interrupt.
    - This stack is a kernel structure (i.e. it is located in the kernel’s address space).
    - It also holds trap frames and switch frames (because it is the kernel that creates trap frames and switch frames)

## Exception Handling
* first to run assembly code, `common_exception`
    - save stack pointer
    - switches SP to point to the thread's kernel stack,
    - carefully saves app state and address of the instruction,
    - calls `mips_trap`, passing a pointer to the trap frame as an arg.
* After `mips_trap` is finished, the `common_exception` handler will
    - restore application's state
    - jump back to application instruction that was interrupted and switch back to unprivileged execution mode.
* See `kern/arch/mips/locore/exception-mips1.S` (assembly file)

### mips_trap
determines what type of exception it is by looking at the exception code.

(there is a separate handler in the kernel for each type of exception.) Based on that exception code, call appropriate handler.
- interrupt? call `mainbus_interrupt`
- address translation exception? `vm_fault`
- system call? call `syscall` (kernel function), passing it the trap
frame pointer
- `kern/arch/mips/syscall/syscall.c`

See `kern/arch/mips/locore/trap.c`

**One important thing**: when exception raised, one of the first thing `common_exception` does it to disable the interrupt on CPU. When we call `mips_trap`, interrupt exceptions are off for now.
- If it was actually an interrupt, they stay off until  the hardware has been handled.
- If not, don't have to call `mainbus_interrupt` to handle it, then we are ok to be interrupted again. It is not hardware interrupt, then we take interrupt exceptions back on.

**increase program counter**: if we don't do this, the system call exception will be raised over and over again. We are halting execution and doing sth else, thus we need manully. 4 byte.

Kernel is just a program, a sequence of instructions.

Two type of calls:
- procedure calls: used by apps to execute other application code,
- sys calls: exec kernel code.

## Multiprocessing
- The OS ensures that *processes are isolated from one another*.
- Interprocess communication should be possible.

Keep in mind, it's not processes that run. Threads execute. Process has to have a thread in order to execute, so we really are talking about multithreads. Now we have a pool of threads, which belong to different processes. Only when quantum expires, we do context switch. (Timer interrupt)

## Stack Diagrams
In the next few minutes, I'm gonna show you stack diagrams because you'd better bet they are going to show up on your exams.

`mips_trap`'s job is to figure out what kind of exception that actually raised. Then call the handler that is specific to this exception. Now we are going to execute `mips_trap`. Now after we check that if it's an interrupt or not, then it realizes that it is not an interrupt, then `mips_trap` will turn exceptions and interrupts back on. It means that while we are executing our sys call, we CAN be interrupted. KEEP THAT IN MIND.

Look at `v0` and that's going to tell you which syscal we actually want to run (a great big old `switch` statement). We want `sys_fork`. Now interrupts are on which means you can be preempted while executed this fork.

So now what happens to our stack: If we have a timer interrupt. First thing you see is trap frame (produced by `common_exception`). Push a trapframe onto the kernel stack. Since we are already in privilege mode and on kernel stack, you actually know the `common_exception` code. If we are on kernel stack, we don't do anything but just save the trapframe right. Then call `mips_trap` to figure out what just happened. It's `mainbus_interrupt`. It's hardware interrupt. While they are handling interrupts from hardware, we don't want to handle other hardware interrupts because this is the behaviour that is outside the control that thread is running and we don't to disrupt it for too long so generally leave it off.

In `mainbus_interrupt`, we discover it's the clock. Call interrupt handler for the clock: exceed schedule quantum, I'd better preempt you. So call `thread_yield` -> `thread_switch` -> `switch_frame`. Context switch occurs. Popping off the stack, then returns to ... Then go back to `common_exception` where we restore the trapframe. Btw, we are turning the interrupts back on. Then back to user stack, reset the CPU back to unprivileged mode and now we go back to have a process to thread running its regular user mode. Similar to multithreads, but we have two stacks.

Now suppose we don't have interrupt. Return to syscall. At the very end of syscall, set return value and success/failure values for that system. Zero: success; One: failure. The last thing in syscall dispatcher（调度）, **increment program counter** because when the exception is raised we didn't actually increment it.
```c++
if (err) { /* err */
    tf->tf_v0 = err;
    tf->tf_a3 = 1;
} else { /* no err */
    tf->tf_v0 = retval;
    tf->tf_a3 = 0;
}

/* advance PC */
tf->tf_epc += 4;
```
`mips_trap` returns to `common_exception`. The trapframe data is
restored. Switch from kernel to user stack. Switch to unprivileged
mode (`rfe`: magical instruction). User code continues execution.

- These diagrams are always on either midterm or final.
- It is not possible for you to have two track frames back-to-back in OS/161. Reason for that is because when the exception is raised at the CPU, interrupts are turned off and they do not come back on again until at least halfway through `mips_trap` which means that each trapframe must be separated by at least `mips_trap`.

## Inter-Process Communication (IPC)
Processes can talk to each other. IPC is a family of methods used to send data between processes.

File, Socket, Pipe, Shared Memory, Message Passing/Queue.

<div class='ex'>
<div class="ex-title">
In-Class Problems: procfork
</div>

<div class='ex-content'>
<p>
Question 1: What output will be generated by the parent and child processes for the program shown
below?
</p>

<pre>
<code>
int x; /* global variable */
int main()
{
    int rc;
    x = 0;
    rc = fork();
    if (rc == 0) {
        x = 10;
        printf("A: %d\n", x);
    } else {
        printf("B: %d\n", x);
        x = 100;
    }
    printf("C: %d\n", x);
}
</code>
</pre>
<p>**Solution**:  
</p>
<ul>
<li>Parent: B: 0, C: 100</li>
<li>Child: A: 10, C: 10</li>
</ul>

<p> Question 2: Consider a system with a process, `ProcA`, that executes the program below. Draw the tree
rooted at `ProcA` representing the processes created by this program. Every node in the tree should represent
a process, and there should be an edge from node A to node B if process A creates process B. Label each
node of the tree with the label of the `fork()` system call that created it.</p>
<pre>
<code>
int main () {
    fork(); /* Label: B */
    fork(); /* Label: C */
    fork(); /* Label: D */
    return 0;
}
</code>
</pre>

<img src="/pics/procfork.svg" width=100%>

<p>Note that the process name here doesn't matter...</p>
</div>
</div>
