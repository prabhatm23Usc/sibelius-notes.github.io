---
title: CS 341 - Algorithms (post mid)
layout: mdtoc
---
post mid material: starting from graph algorithms. Notes are a mix of different offerings...

Better to typeset pseudocode with [pseudocode.js](https://github.com/SaswatPadhi/pseudocode.js/tree/master)...

# Graph Algorithms
## Basic
directed, adjacent, incident, indegree, outdegree, path, cycle, tree, connected, connected component.

A **simple** path does not repeat vertices. A **simple** cycle does not repeat vertices.


Our algorithms will be for abstract graphs. Two ways to represent:
- adjacency matrix: space &#92;(O(n^2) &#92;)
- adjacency lists: space &#92;(O(n+m) &#92;)

|basic operations  | adj matrix| adj lists|
| :------------- | :------------- | :-- |
| list &#92;(v &#92;)'s neighbours| &#92;(\Theta(n) &#92;) | &#92;(\Theta(1+\deg(v)) &#92;)|
| list all edges | &#92;(\Theta(n^2) &#92;) | &#92;(\Theta(n+m) &#92;) |
| is &#92;((u,v)\in E &#92;) | &#92;(\Theta(1) &#92;) | &#92;(\Theta(1+\deg(u)) &#92;) |

where the last complexity &#92;(\Theta(1+\deg(u)) &#92;)  can be reduced to &#92;(O(1) &#92;)
with (sophisiticated) hashing.

For algorithm in this course, we will use adjacency lists.

## Exploring Graphs

### BFS
Cautious search: check everything one edge away, then two...

It takes &#92;(O(n+m) &#92;) time: we explore each vertex once and check all incident edges: &#92;(O(n+\sum_{v\in V} \deg(v))=O(n+m) &#92;). Recall &#92;(\sum_{v\in V} \deg(v) = 2m &#92;).

**Properties**
- The parent pointers create a directed tree.
- &#92;(u &#92;) is connected to &#92;(v_0 &#92;) iff BFS from &#92;(v_0 &#92;) reaches &#92;(u &#92;).
- The level of a vertex &#92;(v &#92;) = length of shortest path from &#92;(v_0 &#92;) to &#92;(v &#92;).

**Consequences**
1. BFS from &#92;(v_0 &#92;) finds the connected component of &#92;(v_0 &#92;)
2. BFS finds shortest paths (# edges) from &#92;(v_0 &#92;)

**BFS to test bipartiteness**
definition omitted... it cannot have an odd cycle.

### DFS
[Extra notes from CMU](https://www.cs.cmu.edu/~15451-f17/lectures/lec11-DFS-strong-components.pdf)

Bold search: go as far as you can; when there's nothing new to discover, retrace your steps to find sth new.

Note: orders depend on order in adjacency list. Use a stack to store vertices that have been discovered but must still be explored. So we use a recursive algorithm where stack is implicit.

Runtime: &#92;(O(n+m) &#92;)

All non-tree edges join ancestor and descendant.

Here we abbr: &#92;(d(v)=discovers(v), f(v)=finish(v) &#92;). If &#92;(d(v)<d(u) &#92;) then we have only two possiblities: either nested or disjoint
```
[       ]       [       ]           OR      [        [       ]       ]
d(v)    f(v)    d(u)    f(u)                d(v)     d(u)    f(u)    f(v)
```
because interval &#92;(d(v),f(v) &#92;) is time on stack.

**DFS to find 2-connected components**. Involve some defns of cut-vertex. Not covered in D.R. Stinson's slide.
- **Claim**: The root is a cut vertex iff it has &#92;(> 1&#92;) child.
- **Lemma**: non-root &#92;(v &#92;) is a cut vertex iff &#92;(v &#92;) has a subtree &#92;(T &#92;) with non-tree edge going to an ancestor of &#92;(v &#92;).

Now let's DFS on **Directed Graphs**.
- **tree edges** form a forest of trees.
- **forward edges** are from a vertex to a descendant in the tree.
- **cross edges** are from node a to node b where the subtrees rooted at a and b are disjoint.
- **back edges** are from a vertex to an ancestor.

Then in DFS, we label the edges:
- if u not finished, then (v, u) is back edge
- else if &#92;(d(u) > d(v)&#92;) then (v, u) forward edge
- else if &#92;(d(u) < d(v) &#92;) then (v, u) cross edge

**Applications of DFS**
1. detecting cycles in directed graphs.<br>
**Lemma**: A directed graph has a directed cycle iff DFS has a back edge.
2. Topological sort of a directed **acyclic** (no directed cycle) graph. Edge &#92;((a,b) &#92;) means &#92;(a &#92;) must come before &#92;(b &#92;). (e.g. job scheduling). Find a linear order of vertices satisfying all edges. (possible if no directed cycle).
<br>**One solution**: find vertex &#92;(v &#92;) with no in-edge. Remove &#92;(v &#92;) and repeat. &#92;(O(n+m) &#92;)
<br>**Solution using DFS**: also &#92;(O(n+m) &#92;) use reverse of finish order.
3. Finding strongly connected components in a directed graph.
<br> Strongly connected = for all vertices &#92;(u,v &#92;) there is a path &#92;(u\to v &#92;) and a path &#92;(v\to u &#92;).
<br> **Claim**  Let &#92;(s &#92;) be a vertex. &#92;(G &#92;) is strongly connected if for all vertices &#92;(v &#92;) there is a path &#92;(s\to v &#92;) and a path &#92;(v\to s &#92;).
<br>To test if there's a path &#92;(s\to v\quad \forall v &#92;), do DFS(s). How can we test it there's a path &#92;(v\to s\quad \forall v &#92;)? Reverse edge directions and do DFS(s). **Neat!**.

More generally, a digraph can be partitioned to strongly connected components.

<img src="/pics/DAG.png">

Picture from [here](https://www.cs.cmu.edu/~15451-f17/lectures/lec11-DFS-strong-components.pdf).

Contracting strongly connected components gives acyclic graph.

*How to find strongly connected components?*

Idea: vertices 1 ... n.
<br>Run DFS (vertex ordering resolves what vertex comes next).
<br> Let finish order be &#92;(f_1 f_2 \ldots f_2 &#92;).
<br>&#92;(G^R=G &#92;) with all edges reversed.
<br>run DFS &#92;(G^R &#92;) with vertex order &#92;(f_n f_{n-1}\ldots f_1 &#92;)

Trees in 2nd DFS are exactly the strongly connected components.

**Sharir's Algorithm to Find the Strongly Connected Components**
1. Perform a depth-first search of G, recording the finishing times *f[v]* for all vertices v.
2. Construct a directed graph H from G by **reversing** the direction of all edges in G.
3. Perform a depth-first search of H, considering the vertices in **decreasing** order of the values *f[v]* computed in step 1.
4. The strongly connected components of G are the trees in the depth-first forest constructed in step 3.

## Minimum Spanning Tree
Given a graph &#92;(G=(V,E) &#92;) with weights &#92;(\omega:E\to \mathbb R^{\ge 0} &#92;) one the edges find a subset of the edges that connected all the vertices and has minimum weight. The edge subset will be a tree, called the **minimum spanning tree**.

Greedy alg will find min spanning trees with different implementation challenges.
- add cheapest edge first, never build a cycle: Kruskal's alg.
- grow connected graph from one vertex: Prim's alg.

### Kruskal's Algorithm
```
Order edges by weight
    w(e_i) <= w(e_{i+1})
T <- ∅
for i = 1..m
    if e_i does not make a cycle with T then
        T <- T ∪ {e_i}
endfor
```
 &#92;(O(m\log m) = O(m\log n) &#92;) to sort edges because &#92;(m\le n^2 &#92;). Then we need to  maintain connected components as we add edges. So it is union-find problem.

#### Union-Find Problem
Maintain a collection of disjoint sets.

Operations:
- `Find(x)` - which set contains element x?
- `Union` - unite two sets.

Every tree contains a leader vertex. We construct an auxiliary array &#92;(L &#92;). From &#92;(v &#92;), follow a directed path &#92;(v\to L[v]\to L[L[v]] \to \ldots &#92;) until we reach &#92;(w &#92;) with &#92;(L[w]=w &#92;). Then &#92;(w=find(v) &#92;).

> Or keep array `S[1..n]`, `S[i]` = component  of element i and keep linked list in each set.

Then `find` is &#92;(O(1) &#92;). Union: must joint 2 linked lists &#92;(O(1) &#92;) and rename one of the two sets. So &#92;(O(n) &#92;) in worst case. BUT renaming smaller set does better! If an element's set number changes, then its set (more than) doubles. This happens &#92;(\le  \log n&#92;)  times. Therefore total renaming work is &#92;(O(n\log n) &#92;). Thus total runtime
&#92;[
    \underbrace{O(m\log n)}_ {sort} +
    \underbrace{O(m)}_ {finds} +
    \underbrace{O(n\log n)}_ {unions}
    = O(m\log n)
&#92;]
Assuming &#92;(G &#92;) is connected, so &#92;(m\ge n-1 &#92;).

### Prim's Algorithm
```
initialize C <- {S}, T <- ∅
while C != V
    find min weight edge e = (u, v) from u ∈ C to v ∈ V \ C
    T <- T ∪ {e}
    C <- C ∪ {v}
end
```


Implementation: We use priority queue as data structure (using min-heap, and we know each operation takes &#92;(O(\log k) &#92;) where &#92;(k &#92;) is # of elements), maitain a set of weighted elements.
- Find and delete min weight element
- insert
- delete

Total cost
&#92;[
    O(n\log m + m\log m+ m\log m)  = O(m\log m) = O(m\log n)
&#92;]

In graph theory, a **cut** is a partition of the vertices of a graph into two disjoint subsets. Any cut determines a cut-set, the set of edges that have one endpoint in each subset of the partition. These edges are said to cross the cut. These edges are called **crossing edges**. Let &#92;(A\subseteq E &#92;). A cut &#92;((S,V\setminus S) &#92;) **respects** the set of edges &#92;(A &#92;) provided that no edges in &#92;(A &#92;) is a crossing edge.

**A general greedy algorithm to find an MST**
```
A <- ∅
while |A| < n-1
    (S, V\S) <- a cit that respects A
    let e be a minimum crossing edge
    A <- A ∪ {e}
end
```

## Single Source Shortest paths
**Instance**: A directed graph &#92;(G=(V,E) &#92;), a non-negative weight function &#92;(w: E\to \mathbb R^{\ge 0} &#92;), and a source vertex &#92;(u_0\in V &#92;).

**Find**: For every vertex &#92;(v\in V &#92;), a directed path &#92;(P &#92;) from &#92;(u_0 &#92;) to &#92;(v &#92;) such that &#92;(w(P)=\sum\limits_{e\in P} w(e)&#92;) is minimized.

### Dijkstra's Algorithm
Dijkstra's algorithm requires that the graph have no edge weights &#92;(< 0 &#92;); it works for directed or undirected graphs.

General step:
```
Have tree of shortest paths to all vertices in set B; initially B = {s}

Choose edge (x,y), x ∈ B, y ∉ B to minimize d(s, x) + w(x, y).
// Here d(s, x) is distance from s to x, which is known.

d(s,y) <- d_min
add (x,y) to tree (Parent(y) <- x)
```

![](/pics/Dijkstra.png)

picture from Prof. Lubiw's F19 CS341 notes. This is greedy in the sense that we always add the vertex with next min distance from s.

Implmentations:
- want to choose edge leaving B to minimize some values
- could make a heap of edges which has size &#92;(O(m) &#92;)
- More effcient: a heap of vertices

Keep "tentative distance" &#92;(d(v) \quad \forall v\not\in B &#92;)

Total time &#92;(O(n\log n + m\log n)=O(m\log n)  &#92;). Here it takes &#92;(n &#92;) to find min, &#92;(m &#92;) to adjust heap. Actually, there is a fancier "Fibonacci heap" that gives &#92;(O(n\log n+m) &#92;).

**Recall**: negative weight cycles are trouble. Note that negative weight edges in an undirected graph are not allowed, as they would give rise to a negative cycle (consisting of two edges) in the associated directed graph.

## Shortest Paths in a DAG
If G is a DAG, we perform a topological ordering of the vertices, the result is &#92;(v_ 1,\ldots, v_ n &#92;). Recall this is done by DFS. If v vomes before s, there is no path s->v. So throw these vertice away.

```
for i = 1 .. n
    for every edge (v_i, v_j)
        if d_i + w(v_i, v_j) < d_j then
            d_j <- d_i + w(v_i, v_j)
    endfor
endfor
```
This is &#92;(O(n+m) &#92;).

### Bellman-Ford
The Bellman-Ford algorithm solves the single source shortest path problem in any directed graph without negative weight cycles.
It is **the** original application of dynamic programming.

**Note**: if there is a negative weight cycle then shortest paths are not well-defined. Go around the cycle move and move to decrease length of path arbitrarily.

**Idea of DP for shortest paths**: If shortest uv path goes through x, then it consists of shortest ux path + shortest xv path (these are subproblems)

Here we want to compute:
&#92;[
    d_i[v] = \text{ length of the shortest path from $s$ to $v$ that uses $\le i$ edges}
&#92;]
base case: &#92;(d_1[v] = &#92;begin{cases}
0 & &#92;text{if } v=s &#92;&#92;
w(s,v) & &#92;text{if } (s,v)\in E  &#92;&#92;
   \infty& &#92;text{otherwise}
&#92;end{cases} &#92;)

and our recurrence:
&#92;[
    d_i(v)= \min &#92;begin{cases}
    d_{i-1}(v) & &#92;text{(use $\le i-1$ edges) }  &#92;&#92;
     \min\limits_{u \in &#92;left&#92;{ u: (u,v)\in E &#92;right&#92;}} (d_{i-1}(u)+w(u,v))  & &#92;text{(use $i$ edges)}
    &#92;end{cases}
&#92;]

![](/pics/bellman1.png)

Picture (as well as some notes below) from Prof. Blais's F19 CS341 notes. This is &#92;(O(n^3) &#92;). The correctness of this algorithm follows directly from the correctness of the Bellman-Ford algorithm. Recall that the Bellman-Ford algorithm is obtained via the dynamic programming technique by considering the shortest paths from &#92;(s &#92;) to &#92;(v &#92;) that go over only &#92;(1,2,\ldots,n-1 &#92;) edges. We saw in the last lecture that we can store these distances in vectors
&#92;(d_1,d_2,\ldots,d_{n-1} &#92;). It turns out that we don't need to store all these distance vectors separately, and we can simplify the Bellman-Ford algorithm to use a single distance vector that
we update as follows.

![](/pics/bellman2.png)

Note the curious fact that &#92;(i &#92;) does not appear inside the loop. So we actually run after some iterations to update some values... Runtime: &#92;(\Theta(nm) &#92;).

However if we use all this algorithm to APSP(All-Pairs Shortest Paths), runtime would be &#92;(O(n^2m) &#92;).

**Conclusion**: To obtain a more ecient algorithm with the dynamic programming
technique, we need tond other subproblems to solve.

## All-Pairs Shortest Paths
In the all-pairs shortest path (APSP)
problem, we are given a weighted (directed or undirected) graph &#92;(G=(V,E) &#92;) and we must
return the length of the shortest path from &#92;(u\to v &#92;) for every &#92;(u,v\in V &#92;).

**A Dynamic Programming Approach**: &#92;(L_m[i,j] &#92;) denotes the minimum-weight &#92;((i,j) &#92;)-path having at most &#92;(m &#92;) edges. For &#92;(m\ge 2 &#92;),
&#92;[
    L_ m[i,j] = \min &#92;left&#92;{ L_ {m-1}[i,k]+ L_ 1[k,j]: 1\le k\le n &#92;right&#92;}
&#92;]
Complexity: &#92;(O(n^4) &#92;).

### Floyd-Warshall
To design the Bellman{Ford algorithm, we noted that the problem of computing the distance
from u to v is much easier if we restrict the set of paths that we consider. This observation
led us to consider only paths that go through a small number of edges. But that's not
the only way to restrict our attention to some of the paths only. In particular, instead of
limiting the number of edges that the paths can use, we can limit the set of intermediate
nodes that those paths can traverse. Let us label the vertices in &#92;(V &#92;) as &#92;(v_1,\ldots,v_n &#92;). Then
for each &#92;(0\le k\le n &#92;), we can define
&#92;[
    dist_ k[u,v] = &#92;begin{aligned}
    \text{minimum weight of any path from $u$ to $v$ that} &#92;&#92;
    \text{only uses &#92;(v_1,\ldots,v_k &#92;) as intermediate nodes.}
    &#92;end{aligned}
&#92;]
Now base case (&#92;(dist_ 0[u,v] &#92;)) is simple: if edge then just the weight, otherwise &#92;(\infty &#92;). Recurrence:
&#92;[
    dist_ k[u,v] = \min &#92;begin{cases}
    dist_ {k-1}[u, v_ k] + dist_ {k-1} [v_ k,v]&#92;&#92;
    dist_ {k-1}[u,v]
    &#92;end{cases}
&#92;]
Combine all these, we get the algorithm with &#92;(\Theta(n^3) &#92;).

![](/pics/floyd.png)

![](/pics/floyd-exercise.png)

# Exhaustive Search

Covered in F19, but doesn't seemed in Prof Stinson's slides...

The notes below are from Prof Blais' notes from F19.

## Systematic search
Let's say you are given the task to solve a comptuational problem where none of the
design techniques we have covered in the class work and--even worse--no-one knows how
to design an ecient algorithm for the problem. What do you do then? There are a few
options.
- Design a *heuristic argument* that runs quickly but has no provable guarantee of correctness.
- Design an *approximation algorithm* that does not solve the problem optimally but at least gives some guarantees about its solution.
- Design an *exponential-time* exact algorithm for the problem.

Here we focus on the last approach.

## Backtracking
<div class="fancy-block" data-type="Problem" data-title="Subset sum">
<div class="fancy-block-content">
We are given as input an array of &#92;(n &#92;) elements &#92;(1,2,\ldots,n &#92;) with positive weights
&#92;(w_1,\ldots,w_n &#92;), alogn with a target weight &#92;(T &#92;). We must determine whether there is a subset
&#92;(S\subseteq &#92;left&#92;{ 1,2,\ldots,n &#92;right&#92;} &#92;) of elements with total weights &#92;(\sum_{i\in S} w_i =T&#92;) or not.
</div></div>

There is no known polynomial-time algorithm for this problem. In fact, it's NP-complete.

However, there are some obvious inefficiencies when we check all possible subsets. For example, if &#92;(w_1>T &#92;), there's no point checking &#92;(S=&#92;left&#92;{ 1,2 &#92;right&#92;} &#92;) or &#92;(&#92;left&#92;{ 1,2,3 &#92;right&#92;} &#92;) and so on.

With the backtracing technique, we can avoid checking all unnecessary cases while still making sure that we don't miss any potential solutions.

```
Initialize the set A of active configurations
while A =/= emptyset do
    C <- next config of A
    if C is a solution return True
    if C is not a dead end then
        Expand(C) and add the resulting config to A
return False
```

## Branch-and-bound
Here is a link to [optimization course](https://ocw.mit.edu/courses/sloan-school-of-management/15-053-optimization-methods-in-management-science-spring-2013/lecture-notes/MIT15_053S13_lec12.pdf). This algorithm will be most likely discussed in CO250/255/353/452...

<div class="fancy-block" data-type="Problem" data-title="Travelling Salesman problem">
<div class="fancy-block-content">
    In the Travelling Salesman problem (TSP), we are given a weighted
undirected graph &#92;(G = (V,E) &#92;) with non-negative edge weights &#92;(w:E\to \mathbb R^{\ge 0} &#92;). Our goal is to
nd a cycle &#92;(C &#92;) that goes through each vertex in &#92;(V &#92;) exactly once (called a TSP tour) with
minimum weight &#92;(\sum_{e\in C}w(e) &#92;).
</div></div>

This is also NP-complete.

# Intractability and Undecidability
## Decision Problems

> defn below from Prof Stinson's slide

- **Decision Problem**: Given a problem instance I, answer a certain question
"yes" or "no".
- **Problem Instance**: Input for the specied problem.
- **Problem Solution**: Correct answer ("yes" or "no") for the specied
problem instance. I is a yes-instance if the correct answer for the
instance I is "yes". I is a no-instance if the correct answer for the
instance I is "no".
- **Size of a problem instance**: Size(I) is the number of bits required to
specify (or encode) the instance I.

<div class="fancy-block" data-type="Definition" data-title="polynomial-time algorithm">
<div class="fancy-block-content">
An algorithm &#92;(A &#92;) is a polynomial-time algorithm if there exists a constant
&#92;(k &#92;) such that on any input of size &#92;(n &#92;), the algorithm A runs in time &#92;(O(n^k) &#92;).
</div></div>

*Which computational problems can be solved by polynomial-time algorithms?*

We will call such problems *tractable*.

*Why polynomial time?*
1. **Robustness**. Any reasonable modication to the model won't aect the tractability or intractability of the problem. (See CS 360/365 for all the details.)
2. **Strength of the conclusion**. So instead of ruling out algorithms in one setting after another in a long and tedious process, we do so all at once.
3. **Applicability**. So knowing how to show that a problem is intractable is not just something that's of purely intellectual interest; it is something that will be useful in practice.

Three types of computational problems:
- **Decision problems**: problems where the possible outputs are True or False
- **Optimization problems**: problems where we are trying tond the value of the best solution.
- **Search problems**: problems where the goal is tond a valid (or the best) solution itself.

## The Complexity Class **P**

<div class="fancy-block" data-type="Definition" data-title="The class P">
<div class="fancy-block-content">
    The class <b>P</b> is the set of all decision problems that can be solved by
polynomial-time algorithms.
</div></div>

How can we show that a new problem is in P?  We can design a polynomial-time algorithm
that solves the problem. Or we can use the idea of reduction.

<div class="fancy-block" data-type="Definition" data-title="polynomial-time reducible">
<div class="fancy-block-content">
The decision problem &#92;(A &#92;)  is polynomial-time reducible to the decision problem &#92;(B &#92;), written
&#92;[
    A \le_{\textbf{P}} B
&#92;]
if there is a polynomial-time algorithm &#92;(F &#92;) that, on any input &#92;(I_A &#92;) to the problem &#92;(A &#92;), produces an input &#92;(I_B &#92;) to the problem &#92;(B &#92;) that has the same answer.
</div></div>

Also Polynomial-time Turing Reductions (not covered in Prof Blais' notes).

Suppose &#92;(\prod_1 &#92;) and &#92;(\prod_2 &#92;) are problems (not necessarily decision problems). A
(hypothetical) algorithm B to solve &#92;(\prod_2 &#92;) is called an **oracle** for &#92;(\prod_2 &#92;).

<div class="fancy-block" data-type="Definition" data-title="Turing reduction">
<div class="fancy-block-content">
<p>Suppose that &#92;(A &#92;) is an algorithm solves &#92;(\prod_1 &#92;), assuming the existence of an oracle &#92;(B &#92;) for &#92;(\prod_2 &#92;). Then we say that &#92;(A &#92;) is a **Turing reduction** from &#92;(\prod_1 &#92;) to &#92;(\prod_2 &#92;), denoted</p>
&#92;[
    \prod_1 \le^T \prod_2.
&#92;]
<p>A Turing reduction &#92;(A &#92;) is **polynomial-time Turing reduction** if the running time of &#92;(A &#92;) is polynomial, under the assumption that the oracle &#92;(B &#92;) has **unit cose** running time.
</p>
<p>If there is a polynomial-time Turing reduction from &#92;(\prod_1 &#92;) to &#92;(\prod_2 &#92;), we write </p>
&#92;[
    \prod_1 \le^T_P \prod_2.
&#92;]
</div></div>
