---
title: CS 341 - Algorithms (post mid)
layout: mdtoc
---
post mid material: starting from graph algorithms. Notes are a mix of different offerings...

Better to typeset pseudocode with [pseudocode.js](https://github.com/SaswatPadhi/pseudocode.js/tree/master)...

# Graph Algorithms
## Basic
directed, adjacent, incident, indegree, outdegree, path, cycle, tree, connected, connected component.

A **simple** path does not repeat vertices. A **simple** cycle does not repeat vertices.


Our algorithms will be for abstract graphs. Two ways to represent:
- adjacency matrix: space &#92;(O(n^2) &#92;)
- adjacency lists: space &#92;(O(n+m) &#92;)

|basic operations  | adj matrix| adj lists|
| :------------- | :------------- | :-- |
| list &#92;(v &#92;)'s neighbours| &#92;(\Theta(n) &#92;) | &#92;(\Theta(1+\deg(v)) &#92;)|
| list all edges | &#92;(\Theta(n^2) &#92;) | &#92;(\Theta(n+m) &#92;) |
| is &#92;((u,v)\in E &#92;) | &#92;(\Theta(1) &#92;) | &#92;(\Theta(1+\deg(u)) &#92;) |

where the last complexity &#92;(\Theta(1+\deg(u)) &#92;)  can be reduced to &#92;(O(1) &#92;)
with (sophisiticated) hashing.

For algorithm in this course, we will use adjacency lists.

## Exploring Graphs

### BFS
Cautious search: check everything one edge away, then two...

It takes &#92;(O(n+m) &#92;) time: we explore each vertex once and check all incident edges: &#92;(O(n+\sum_{v\in V} \deg(v))=O(n+m) &#92;). Recall &#92;(\sum_{v\in V} \deg(v) = 2m &#92;).

**Properties**
- The parent pointers create a directed tree.
- &#92;(u &#92;) is connected to &#92;(v_0 &#92;) iff BFS from &#92;(v_0 &#92;) reaches &#92;(u &#92;).
- The level of a vertex &#92;(v &#92;) = length of shortest path from &#92;(v_0 &#92;) to &#92;(v &#92;).

**Consequences**
1. BFS from &#92;(v_0 &#92;) finds the connected component of &#92;(v_0 &#92;)
2. BFS finds shortest paths (# edges) from &#92;(v_0 &#92;)

**BFS to test bipartiteness**
definition omitted... it cannot have an odd cycle.

### DFS
[Extra notes from CMU](https://www.cs.cmu.edu/~15451-f17/lectures/lec11-DFS-strong-components.pdf)

Bold search: go as far as you can; when there's nothing new to discover, retrace your steps to find sth new.

Note: orders depend on order in adjacency list. Use a stack to store vertices that have been discovered but must still be explored. So we use a recursive algorithm where stack is implicit.

Runtime: &#92;(O(n+m) &#92;)

All non-tree edges join ancestor and descendant.

Here we abbr: &#92;(d(v)=discovers(v), f(v)=finish(v) &#92;). If &#92;(d(v)<d(u) &#92;) then we have only two possiblities: either nested or disjoint
```
[       ]       [       ]           OR      [        [       ]       ]
d(v)    f(v)    d(u)    f(u)                d(v)     d(u)    f(u)    f(v)
```
because interval &#92;(d(v),f(v) &#92;) is time on stack.

**DFS to find 2-connected components**. Involve some defns of cut-vertex. Not covered in D.R. Stinson's slide.
- **Claim**: The root is a cut vertex iff it has &#92;(> 1&#92;) child.
- **Lemma**: non-root &#92;(v &#92;) is a cut vertex iff &#92;(v &#92;) has a subtree &#92;(T &#92;) with non-tree edge going to an ancestor of &#92;(v &#92;).

Now let's DFS on **Directed Graphs**.
- **tree edges** form a forest of trees.
- **forward edges** are from a vertex to a descendant in the tree.
- **cross edges** are from node a to node b where the subtrees rooted at a and b are disjoint.
- **back edges** are from a vertex to an ancestor.

Then in DFS, we label the edges:
- if u not finished, then (v, u) is back edge
- else if &#92;(d(u) > d(v)&#92;) then (v, u) forward edge
- else if &#92;(d(u) < d(v) &#92;) then (v, u) cross edge

**Applications of DFS**
1. detecting cycles in directed graphs.<br>
**Lemma**: A directed graph has a directed cycle iff DFS has a back edge.
2. Topological sort of a directed **acyclic** (no directed cycle) graph. Edge &#92;((a,b) &#92;) means &#92;(a &#92;) must come before &#92;(b &#92;). (e.g. job scheduling). Find a linear order of vertices satisfying all edges. (possible if no directed cycle).
<br>**One solution**: find vertex &#92;(v &#92;) with no in-edge. Remove &#92;(v &#92;) and repeat. &#92;(O(n+m) &#92;)
<br>**Solution using DFS**: also &#92;(O(n+m) &#92;) use reverse of finish order.
3. Finding strongly connected components in a directed graph.
<br> Strongly connected = for all vertices &#92;(u,v &#92;) there is a path &#92;(u\to v &#92;) and a path &#92;(v\to u &#92;).
<br> **Claim**  Let &#92;(s &#92;) be a vertex. &#92;(G &#92;) is strongly connected if for all vertices &#92;(v &#92;) there is a path &#92;(s\to v &#92;) and a path &#92;(v\to s &#92;).
<br>To test if there's a path &#92;(s\to v\quad \forall v &#92;), do DFS(s). How can we test it there's a path &#92;(v\to s\quad \forall v &#92;)? Reverse edge directions and do DFS(s). **Neat!**.

More generally, a digraph can be partitioned to strongly connected components.

<img src="/pics/DAG.png">

Picture from [here](https://www.cs.cmu.edu/~15451-f17/lectures/lec11-DFS-strong-components.pdf).

Contracting strongly connected components gives acyclic graph.

*How to find strongly connected components?*

Idea: vertices 1 ... n.
<br>Run DFS (vertex ordering resolves what vertex comes next).
<br> Let finish order be &#92;(f_1 f_2 \ldots f_2 &#92;).
<br>&#92;(G^R=G &#92;) with all edges reversed.
<br>run DFS &#92;(G^R &#92;) with vertex order &#92;(f_n f_{n-1}\ldots f_1 &#92;)

Trees in 2nd DFS are exactly the strongly connected components.

**Sharir's Algorithm to Find the Strongly Connected Components**
1. Perform a depth-first search of G, recording the finishing times *f[v]* for all vertices v.
2. Construct a directed graph H from G by **reversing** the direction of all edges in G.
3. Perform a depth-first search of H, considering the vertices in **decreasing** order of the values *f[v]* computed in step 1.
4. The strongly connected components of G are the trees in the depth-first forest constructed in step 3.

## Minimum Spanning Tree
Given a graph &#92;(G=(V,E) &#92;) with weights &#92;(\omega:E\to \mathbb R^{\ge 0} &#92;) one the edges find a subset of the edges that connected all the vertices and has minimum weight. The edge subset will be a tree, called the **minimum spanning tree**.

Greedy alg will find min spanning trees with different implementation challenges.
- add cheapest edge first, never build a cycle: Kruskal's alg.
- grow connected graph from one vertex: Prim's alg.

### Kruskal's Algorithm
```
Order edges by weight
    w(e_i) <= w(e_{i+1})
T <- ∅
for i = 1..m
    if e_i does not make a cycle with T then
        T <- T ∪ {e_i}
endfor
```
 &#92;(O(m\log m) = O(m\log n) &#92;) to sort edges because &#92;(m\le n^2 &#92;). Then we need to  maintain connected components as we add edges. So it is union-find problem.

#### Union-Find Problem
Maintain a collection of disjoint sets.

Operations:
- `Find(x)` - which set contains element x?
- `Union` - unite two sets.

Every tree contains a leader vertex. We construct an auxiliary array &#92;(L &#92;). From &#92;(v &#92;), follow a directed path &#92;(v\to L[v]\to L[L[v]] \to \ldots &#92;) until we reach &#92;(w &#92;) with &#92;(L[w]=w &#92;). Then &#92;(w=find(v) &#92;).

> Or keep array `S[1..n]`, `S[i]` = component  of element i and keep linked list in each set.

Then `find` is &#92;(O(1) &#92;). Union: must joint 2 linked lists &#92;(O(1) &#92;) and rename one of the two sets. So &#92;(O(n) &#92;) in worst case. BUT renaming smaller set does better! If an element's set number changes, then its set (more than) doubles. This happens &#92;(\le  \log n&#92;)  times. Therefore total renaming work is &#92;(O(n\log n) &#92;). Thus total runtime
&#92;[
    \underbrace{O(m\log n)}_ {sort} +
    \underbrace{O(m)}_ {finds} +
    \underbrace{O(n\log n)}_ {unions}
    = O(m\log n)
&#92;]
Assuming &#92;(G &#92;) is connected, so &#92;(m\ge n-1 &#92;).

### Prim's Algorithm
```
initialize C <- {S}, T <- ∅
while C != V
    find min weight edge e = (u, v) from u ∈ C to v ∈ V \ C
    T <- T ∪ {e}
    C <- C ∪ {v}
end
```


Implementation: We use priority queue as data structure (using min-heap, and we know each operation takes &#92;(O(\log k) &#92;) where &#92;(k &#92;) is # of elements), maitain a set of weighted elements.
- Find and delete min weight element
- insert
- delete

Total cost
&#92;[
    O(n\log m + m\log m+ m\log m)  = O(m\log m) = O(m\log n)
&#92;]

In graph theory, a **cut** is a partition of the vertices of a graph into two disjoint subsets. Any cut determines a cut-set, the set of edges that have one endpoint in each subset of the partition. These edges are said to cross the cut. These edges are called **crossing edges**. Let &#92;(A\subseteq E &#92;). A cut &#92;((S,V\setminus S) &#92;) **respects** the set of edges &#92;(A &#92;) provided that no edges in &#92;(A &#92;) is a crossing edge.

**A general greedy algorithm to find an MST**
```
A <- ∅
while |A| < n-1
    (S, V\S) <- a cit that respects A
    let e be a minimum crossing edge
    A <- A ∪ {e}
end
```

## Single Source Shortest paths
**Instance**: A directed graph &#92;(G=(V,E) &#92;), a non-negative weight function &#92;(w: E\to \mathbb R^{\ge 0} &#92;), and a source vertex &#92;(u_0\in V &#92;).

**Find**: For every vertex &#92;(v\in V &#92;), a directed path &#92;(P &#92;) from &#92;(u_0 &#92;) to &#92;(v &#92;) such that &#92;(w(P)=\sum\limits_{e\in P} w(e)&#92;) is minimized.

### Dijkstra's Algorithm
Dijkstra's algorithm requires that the graph have no edge weights &#92;(< 0 &#92;); it works for directed or undirected graphs.

General step:
```
Have tree of shortest paths to all vertices in set B; initially B = {s}

Choose edge (x,y), x ∈ B, y ∉ B to minimize d(s, x) + w(x, y).
// Here d(s, x) is distance from s to x, which is known.

d(s,y) <- d_min
add (x,y) to tree (Parent(y) <- x)
```

![](/pics/Dijkstra.png)

picture from Prof. Lubiw's F19 CS341 notes. This is greedy in the sense that we always add the vertex with next min distance from s.

Implmentations:
- want to choose edge leaving B to minimize some values
- could make a heap of edges which has size &#92;(O(m) &#92;)
- More effcient: a heap of vertices

Keep "tentative distance" &#92;(d(v) \quad \forall v\not\in B &#92;)

Total time &#92;(O(n\log n + m\log n)=O(m\log n)  &#92;). Here it takes &#92;(n &#92;) to find min, &#92;(m &#92;) to adjust heap. Actually, there is a fancier "Fibonacci heap" that gives &#92;(O(n\log n+m) &#92;).

**Recall**: negative weight cycles are trouble. Note that negative weight edges in an undirected graph are not allowed, as they would give rise to a negative cycle (consisting of two edges) in the associated directed graph.

## Shortest Paths in a DAG
If G is a DAG, we perform a topological ordering of the vertices, the result is &#92;(v_ 1,\ldots, v_ n &#92;). Recall this is done by DFS. If v vomes before s, there is no path s->v. So throw these vertice away.

```
for i = 1 .. n
    for every edge (v_i, v_j)
        if d_i + w(v_i, v_j) < d_j then
            d_j <- d_i + w(v_i, v_j)
    endfor
endfor
```
This is &#92;(O(n+m) &#92;).

### Bellman-Ford
The Bellman-Ford algorithm solves the single source shortest path problem in any directed graph without negative weight cycles.
It is **the** original application of dynamic programming.

**Note**: if there is a negative weight cycle then shortest paths are not well-defined. Go around the cycle move and move to decrease length of path arbitrarily.

**Idea of DP for shortest paths**: If shortest uv path goes through x, then it consists of shortest ux path + shortest xv path (these are subproblems)

Here we want to compute:
&#92;[
    d_i[v] = \text{ length of the shortest path from $s$ to $v$ that uses $\le i$ edges}
&#92;]
base case: &#92;(d_1[v] = &#92;begin{cases}
0 & &#92;text{if } v=s &#92;&#92;
w(s,v) & &#92;text{if } (s,v)\in E  &#92;&#92;
   \infty& &#92;text{otherwise}
&#92;end{cases} &#92;)

and our recurrence:
&#92;[
    d_i(v)= \min &#92;begin{cases}
    d_{i-1}(v) & &#92;text{(use $\le i-1$ edges) }  &#92;&#92;
     \min\limits_{u \in &#92;left&#92;{ u: (u,v)\in E &#92;right&#92;}} (d_{i-1}(u)+w(u,v))  & &#92;text{(use $i$ edges)}
    &#92;end{cases}
&#92;]

![](/pics/bellman1.png)

Picture (as well as some notes below) from Prof. Blais's F19 CS341 notes. This is &#92;(O(n^3) &#92;). The correctness of this algorithm follows directly from the correctness of the Bellman-Ford algorithm. Recall that the Bellman-Ford algorithm is obtained via the dynamic programming technique by considering the shortest paths from &#92;(s &#92;) to &#92;(v &#92;) that go over only &#92;(1,2,\ldots,n-1 &#92;) edges. We saw in the last lecture that we can store these distances in vectors
&#92;(d_1,d_2,\ldots,d_{n-1} &#92;). It turns out that we don't need to store all these distance vectors separately, and we can simplify the Bellman-Ford algorithm to use a single distance vector that
we update as follows.

![](/pics/bellman2.png)

Note the curious fact that &#92;(i &#92;) does not appear inside the loop. So we actually run after some iterations to update some values... Runtime: &#92;(\Theta(nm) &#92;).

However if we use all this algorithm to APSP(All-Pairs Shortest Paths), runtime would be &#92;(O(n^2m) &#92;).

**Conclusion**: To obtain a more ecient algorithm with the dynamic programming
technique, we need tond other subproblems to solve.

## All-Pairs Shortest Paths
In the all-pairs shortest path (APSP)
problem, we are given a weighted (directed or undirected) graph &#92;(G=(V,E) &#92;) and we must
return the length of the shortest path from &#92;(u\to v &#92;) for every &#92;(u,v\in V &#92;).

**A Dynamic Programming Approach**: &#92;(L_m[i,j] &#92;) denotes the minimum-weight &#92;((i,j) &#92;)-path having at most &#92;(m &#92;) edges. For &#92;(m\ge 2 &#92;),
&#92;[
    L_ m[i,j] = \min &#92;left&#92;{ L_ {m-1}[i,k]+ L_ 1[k,j]: 1\le k\le n &#92;right&#92;}
&#92;]
Complexity: &#92;(O(n^4) &#92;).

### Floyd-Warshall
To design the Bellman{Ford algorithm, we noted that the problem of computing the distance
from u to v is much easier if we restrict the set of paths that we consider. This observation
led us to consider only paths that go through a small number of edges. But that's not
the only way to restrict our attention to some of the paths only. In particular, instead of
limiting the number of edges that the paths can use, we can limit the set of intermediate
nodes that those paths can traverse. Let us label the vertices in &#92;(V &#92;) as &#92;(v_1,\ldots,v_n &#92;). Then
for each &#92;(0\le k\le n &#92;), we can define
&#92;[
    dist_ k[u,v] = &#92;begin{aligned}
    \text{minimum weight of any path from $u$ to $v$ that} &#92;&#92;
    \text{only uses &#92;(v_1,\ldots,v_k &#92;) as intermediate nodes.}
    &#92;end{aligned}
&#92;]
Now base case (&#92;(dist_ 0[u,v] &#92;)) is simple: if edge then just the weight, otherwise &#92;(\infty &#92;). Recurrence:
&#92;[
    dist_ k[u,v] = \min &#92;begin{cases}
    dist_ {k-1}[u, v_ k] + dist_ {k-1} [v_ k,v]&#92;&#92;
    dist_ {k-1}[u,v]
    &#92;end{cases}
&#92;]
Combine all these, we get the algorithm with &#92;(\Theta(n^3) &#92;).

![](/pics/floyd.png)

![](/pics/floyd-exercise.png)

# Exhaustive Search

Covered in F19, but doesn't seemed in Prof Stinson's slides...

The notes below are from Prof Blais' notes from F19.

## Systematic search
Let's say you are given the task to solve a comptuational problem where none of the
design techniques we have covered in the class work and--even worse--no-one knows how
to design an ecient algorithm for the problem. What do you do then? There are a few
options.
- Design a *heuristic argument* that runs quickly but has no provable guarantee of correctness.
- Design an *approximation algorithm* that does not solve the problem optimally but at least gives some guarantees about its solution.
- Design an *exponential-time* exact algorithm for the problem.

Here we focus on the last approach.

## Backtracking
<div class="fancy-block" data-type="Problem" data-title="Subset sum">
<div class="fancy-block-content">
We are given as input an array of &#92;(n &#92;) elements &#92;(1,2,\ldots,n &#92;) with positive weights
&#92;(w_1,\ldots,w_n &#92;), alogn with a target weight &#92;(T &#92;). We must determine whether there is a subset
&#92;(S\subseteq &#92;left&#92;{ 1,2,\ldots,n &#92;right&#92;} &#92;) of elements with total weights &#92;(\sum_{i\in S} w_i =T&#92;) or not.
</div></div>

There is no known polynomial-time algorithm for this problem. In fact, it's NP-complete.

However, there are some obvious inefficiencies when we check all possible subsets. For example, if &#92;(w_1>T &#92;), there's no point checking &#92;(S=&#92;left&#92;{ 1,2 &#92;right&#92;} &#92;) or &#92;(&#92;left&#92;{ 1,2,3 &#92;right&#92;} &#92;) and so on.

With the backtracing technique, we can avoid checking all unnecessary cases while still making sure that we don't miss any potential solutions.

```
Initialize the set A of active configurations
while A =/= emptyset do
    C <- next config of A
    if C is a solution return True
    if C is not a dead end then
        Expand(C) and add the resulting config to A
return False
```

## Branch-and-bound
Here is a link to [optimization course](https://ocw.mit.edu/courses/sloan-school-of-management/15-053-optimization-methods-in-management-science-spring-2013/lecture-notes/MIT15_053S13_lec12.pdf). This algorithm will be most likely discussed in CO250/255/353/452...

<div class="fancy-block" data-type="Problem" data-title="Travelling Salesman problem">
<div class="fancy-block-content">
    In the Travelling Salesman problem (TSP), we are given a weighted
undirected graph &#92;(G = (V,E) &#92;) with non-negative edge weights &#92;(w:E\to \mathbb R^{\ge 0} &#92;). Our goal is to
nd a cycle &#92;(C &#92;) that goes through each vertex in &#92;(V &#92;) exactly once (called a TSP tour) with
minimum weight &#92;(\sum_{e\in C}w(e) &#92;).
</div></div>

This is also NP-complete.

# Intractability and Undecidability
## Decision Problems

> defn below from Prof Stinson's slide

Here I is input instance...
- **Decision Problem**: Given a problem instance I, answer a certain question
"yes" or "no".
- **Problem Instance**: Input for the specied problem.
- **Problem Solution**: Correct answer ("yes" or "no") for the specied
problem instance. I is a yes-instance if the correct answer for the
instance I is "yes". I is a no-instance if the correct answer for the
instance I is "no".
- **Size of a problem instance**: Size(I) is the number of bits required to
specify (or encode) the instance I.

<div class="fancy-block" data-type="Definition" data-title="polynomial-time algorithm">
<div class="fancy-block-content">
An algorithm &#92;(A &#92;) is a polynomial-time algorithm if there exists a constant
&#92;(k &#92;) such that on any input of size &#92;(n &#92;), the algorithm A runs in time &#92;(O(n^k) &#92;).
</div></div>

*Which computational problems can be solved by polynomial-time algorithms?*

We will call such problems *tractable*.

*Why polynomial time?*
1. **Robustness**. Any reasonable modication to the model won't aect the tractability or intractability of the problem. (See CS 360/365 for all the details.)
2. **Strength of the conclusion**. So instead of ruling out algorithms in one setting after another in a long and tedious process, we do so all at once.
3. **Applicability**. So knowing how to show that a problem is intractable is not just something that's of purely intellectual interest; it is something that will be useful in practice.

Three types of computational problems:
- **Decision problems**: problems where the possible outputs are True or False
- **Optimization problems**: problems where we are trying tond the value of the best solution.
- **Search problems**: problems where the goal is tond a valid (or the best) solution itself.

## The Complexity Class **P**

<div class="fancy-block" data-type="Definition" data-title="The class P">
<div class="fancy-block-content">
    The class <b>P</b> is the set of all decision problems that can be solved by
polynomial-time algorithms.
</div></div>

How can we show that a new problem is in P?  We can design a polynomial-time algorithm
that solves the problem. Or we can use the idea of reduction.

## Reduction

<div class="fancy-block" data-type="Definition" data-title="polynomial-time reducible">
<div class="fancy-block-content">
The decision problem &#92;(A &#92;)  is polynomial-time reducible to the decision problem &#92;(B &#92;), written
&#92;[
    A \le_{\textbf{P}} B
&#92;]
if there is a polynomial-time algorithm &#92;(F &#92;) that, on any input &#92;(I_A &#92;) to the problem &#92;(A &#92;), produces an input &#92;(I_B &#92;) to the problem &#92;(B &#92;) that has the same answer.
</div></div>

In other words, A is *polynomial-time reducible* to B if there is a *polynomial-transformation*
of the inputs of A to those of B that maps all Yes inputs of A to the Yes inputs of B and all
the No inputs of A to the No inputs of B.

This defn comes from Prof Blais, he didn't make a clear distinction with different type of reductions. See discussion below from Prof Stinson.

Suppose &#92;(\Pi_1 &#92;) and &#92;(\Pi_2 &#92;) are problems (not necessarily decision problems). A
(hypothetical) algorithm B to solve &#92;(\Pi_2 &#92;) is called an **oracle** for &#92;(\Pi_2 &#92;).
(&#92;(B &#92;)is used as a subroutine within the algorithm &#92;(A &#92;).)
<div class="fancy-block" data-type="Definition" data-title="Turing reduction">
<div class="fancy-block-content">
<p>Suppose that &#92;(A &#92;) is an algorithm solves &#92;(\Pi_1 &#92;), assuming the existence of an oracle &#92;(B &#92;) for &#92;(\Pi_2 &#92;). Then we say that &#92;(A &#92;) is a <b>Turing reduction</b> from &#92;(\Pi_1 &#92;) to &#92;(\Pi_2 &#92;), denoted</p>
&#92;[
    \Pi_1 \le^T \Pi_2.
&#92;]
<p>A Turing reduction &#92;(A &#92;) is <b>polynomial-time Turing reduction</b> if the running time of &#92;(A &#92;) is polynomial, under the assumption that the oracle &#92;(B &#92;) has <b>unit cost</b> running time.
</p>
<p>If there is a polynomial-time Turing reduction from &#92;(\Pi_1 &#92;) to &#92;(\Pi_2 &#92;), we write </p>
&#92;[
    \Pi_1 \le^T_P \Pi_2.
&#92;]
</div></div>

**Note**: Prof Blais didn't introduce Turing concept here. So from Prof Stinson's slide, or from [wiki](https://en.wikipedia.org/wiki/Polynomial-time_reduction):
- **Many-one reductions**. Polynomial-time many-one reductions may also be known as **polynomial transformations** or **Karp reductions**, named after Richard Karp. Denoted &#92;(\Pi_1\le_P \Pi_2 &#92;).
- Truth-table reductions. See wiki for more details...
- **Turing reductions**. Many-one reductions can be regarded as restricted variants of Turing reductions where the number of calls made to the subroutine for problem B is exactly one and the value returned by the reduction is the same value as the one returned by the subroutine. So  &#92;(\Pi_1\le_P \Pi_2\implies \Pi_1\le_P^T \Pi_2  &#92;).

<div class="fancy-block" data-type="Problem" data-title="Clique">
<div class="fancy-block-content">
    An undirected graph &#92;(G &#92;) and postive integer &#92;(k &#92;), determine if &#92;(G &#92;) has a clique of size at least &#92;(k &#92;). (A <b>clique</b> is a subset of vertices that any two pair of vertices are adjacent, induced subgraph is complete.)
</div></div>

<div class="fancy-block" data-type="Problem" data-title="IndepSet">
<div class="fancy-block-content">
    Determine if &#92;(G &#92;) has an independent set of size at least &#92;(k &#92;). (An <b>independent set</b> is a set of vertices, no two joined by an edge.)
</div></div>

<div class="fancy-block" data-type="Problem" data-title="VertexCover">
<div class="fancy-block-content">
    Determine if &#92;(G &#92;) has a vertex cover of size at most &#92;(k &#92;).
</div></div>

See the defn for vertex cover in [CO 342](/1195/co342.pdf).

<div class="fancy-block" data-type="Problem" data-title="NonEmpty">
<div class="fancy-block-content">
    Determine &#92;(G &#92;) has at least one edge.
</div></div>

By taking the complement, we can prove &#92;({Clique} \le_P {IndepSet} &#92;) and &#92;({Clique}\le_P {IndepSet} &#92;).

By finding a clique of size 2, &#92;({NonEmpty} \le_P {Clique} &#92;). However, &#92;({Clique} \le_P {NonEmpty} &#92;)? is an open question that is in fact equivalent to the famous P vs. NP problem.

Consider polynomial-time algorithm &#92;(F &#92;) that transforms the input &#92;((G,k) &#92;) to &#92;({IndepSet} &#92;) into the input &#92;((G,n-k) &#92;) to &#92;({VertexCover} &#92;). Then we can prove that &#92;({IndepSet} \le_P {VertexCover} &#92;).

<div class="fancy-block" data-type="Problem" data-title="SetCover">
<div class="fancy-block-content">
Given a collection of &#92;(\mathcal S &#92;) of subsets of &#92;(&#92;left&#92;{ 1,2\ldots,m &#92;right&#92;} &#92;) and a postive integer &#92;(k &#92;), determine if there are &#92;(k &#92;) sets &#92;(S_1,\ldots,S_k\in \mathcal S &#92;) such that &#92;(S_1\cup \ldots \cup S_k = &#92;left&#92;{ 1,2,\ldots,m &#92;right&#92;} &#92;).
</div></div>

Then &#92;({VertexCover} \le_P {SetCover} &#92;). This proof is interesting... So we for each vertex &#92;(v\in V &#92;) we create the set
&#92;[
    S_v = &#92;left&#92;{ i\le m: v \text{ is incident to edge $i$ in $G$} &#92;right&#92;}.
&#92;]

There is one similar assignment question in CO 342: if &#92;(G &#92;) is &#92;(k &#92;)-connected then it is also &#92;(k &#92;)-edge-connected.

<div class="fancy-block"  data-type="Theorem">
<div class="fancy-block-content">
    Two decision problems, &#92;(\Pi_1\le_P \Pi_2 &#92;) and &#92;(\Pi_2\in {\bf P} &#92;), then &#92;(\Pi_1\in P &#92;).
</div></div>

<div class="fancy-block" data-type="Theorem">
<div class="fancy-block-content">
    If &#92;(A,B &#92;) and &#92;(C &#92;) are decision problems that satisfy &#92;(A\le_P B &#92;) and &#92;(B\le_P C &#92;), then &#92;(A\le_P C &#92;).
</div></div>
Thus we can have a chain
&#92;[{Clique} \le_P {IndepSet} \le_P {VertexCover} \le_P {SetCover} &#92;]

## The Complexity Class **NP**
Some definitions...

If the answer is YES to decision problem then there is some succint info (certificate) to verify it.

Informally, a certificate for a yes-instance &#92;(I &#92;) is some "extra information" &#92;(C &#92;) which makes it easy to **verify** that &#92;(I &#92;) is a yes-instance.

**Certificate Verification Algorithm**: Suppose that `Ver` is an algorithm
that verifies certifcates for yes-instances. Then `Ver(I,C)` outputs "yes" if
I is a yes-instance and C is a valid certificate for I. If `Ver(I,C)` outputs
"no", then either I is a no-instance, or I is a yes-instance and C is an
invalid certificate.

**Polynomial-time Certificate Verification Algorithm**: A certificate
verification algorithm `Ver` is a polynomial-time certificate verification
algorithm if the complexity of `Ver` is &#92;(O(n^k) &#92;), where &#92;(k &#92;) is a postive integer
and &#92;(n = Size(I) &#92;).

Formally...

<div class="fancy-block" data-type="Definition" data-title="Certificate Verification Algorithm">
<div class="fancy-block-content">
A certificate verification algorithm <code>Ver</code> is said to solve a decision problem &#92;(\Pi &#92;) provided that
<ul>
<li>for every &#92;(I_{yes}&#92;), there exists a certificate &#92;(C &#92;) such that <code>Ver(I,C)</code> outputs yes.</li>
<li>&#92;(I_{no} &#92;), ... outputs no.</li>
</ul>
</div></div>

<div class="fancy-block" data-type="Definition" data-title="The Complexity Class NP">
<div class="fancy-block-content">
<b>NP</b>(nondeterministic polynomial-time) denotes the set of all decision problems that
have polynomial-time certificate verication algorithms solving them. We
write &#92;(\Pi\in {\bf NP} &#92;) if the decision problem &#92;(\Pi &#92;) is in the complexity class &#92;({\bf NP} &#92;).
</div></div>

It is **not required** to be able to *find* a certificate &#92;(C &#92;) for a yes-instance in polynomial time in order to say that a decision problem &#92;(\Pi\in {\bf NP} &#92;).

**Important Fact**: &#92;({\bf P}\subseteq {\bf NP} &#92;).

Why bother with certifcates? Here is a discussion on certificate for Simplex Algorithm from piazza in CO250. Credit to Prof Joseph Cheriyan.
> All certificates in CO250 are in terms of the original data, so when our algorithm returns an answer it also returns a certificate (in terms of the original data); this allows the "user" (who fed the input to the algorithm) to verify the algorithm's answer (without trusting the working of the algorithm).<br><br>
This is a seemingly small idea, but in fact, it is fundamental to subjects such as optimization, NP-completeness, cryptography, etc.
... To summarize: Certificates are fundamental to CO250 (as well as many other subjects in Optimization and Computer Science) -- but there are other areas, such as ML, where the notion of certificates has no role at present.

and coNP
&#92;[
    \displaystyle \mathbf {coNP} :=\{L|L^C\in \mathbf {NP} \}
&#92;]
So it is a set of decision probelsm where the NO instances can be verified in polynomial time.

So we have &#92;({\bf P\subseteq NP, \quad P\subseteq coNP} &#92;).

## **NP**-Completeness
<div class="fancy-block" data-type="Definition" data-title="NP-complete">
<div class="fancy-block-content">
    A decision problem &#92;(X &#92;) is NP-complete if
    <ul>
    <li>&#92;(X\in {\bf NP} &#92;)</li>
    <li>For every &#92;(Y\in {\bf NP} &#92;), &#92;(Y\le_P X &#92;).</li>
    </ul>
i.e. &#92;(X &#92;) is [one of] the hardest problems in NP.
</div></div>

In Prof Stinson's slide, **NP**-complete is abbr as **NPC**.

Note that the denition does not imply that NP-complete problems exist!

<div class="fancy-block"  data-type="Theorem">
<div class="fancy-block-content">
    (from Prof Blais). Let X be any NPC. Then
    &#92;[
        &#92;begin{cases}
        {\bf P=NP} & &#92;text{if } X\in {\bf P} &#92;&#92;
         {\bf P\ne NP}  & &#92;text{otherwise}
        &#92;end{cases}
    &#92;]
</div></div>

<div class="fancy-block"  data-type="Theorem">
<div class="fancy-block-content">
    (from Prof Lubiw). X is NPC
    <ul>
<li>&#92;(X\in {\bf P \implies P=NP  }&#92;)</li>
<li>If &#92;(X &#92;) cannot be solved in poly time, then no NPC can be solved in polytime.</li>
<li>If &#92;(X\in {\bf coNP} &#92;), then &#92;({\bf NP=coNP} &#92;) (this needs proof).</li>
    </ul>
</div></div>

The first NPC proof is difficult: must show that every problem &#92;(Y\in {\bf NP} &#92;) reduces to &#92;({\bf X} &#92;).
But subsequence NPC proofs are easier, because &#92;(\le_P &#92;) is transtive. Thus to prove &#92;(Z \in {\bf NPC}&#92;):
1. Prove &#92;(Z\in NP &#92;).
2. Prove &#92;(X\le_P Z &#92;) for known NP-complete problem &#92;(X &#92;).

### NP-complete Problems
<div class="fancy-block" data-type="Problem" data-title="CNF-Satisability">
<div class="fancy-block-content">
A boolean formula &#92;(F &#92;) in &#92;(n &#92;) boolean variables &#92;(x_1,\ldots,x_n &#92;), such that &#92;(F &#92;) is the conjunction of &#92;(m &#92;) clauses, where each clause is the disjunction of literals. (A literal is a boolean variable or its negation).
<br><br>
Is there a truth assignment such that &#92;(F &#92;) to evaluates to be <span style="color:blue">true</span>?
</div></div>

<div class="fancy-block" data-type="Theorem" data-title="Cook-Levin Theorem">
<div class="fancy-block-content">
&#92;[
    {\bf CNF-Satisability\in NPC }
&#92;]
</div></div>

As abbr in Prof Blais' note, we use SAT and 3SAT later on. Here 3SAT: each clause has at most 3 literals (or exactly... from Stinson's.) Here we can show &#92;({\bf SAT \le_P 3SAT} &#92;).

With some transformation in polytime, and together with its **NP** and chain relation, we can show &#92;(Clique, IndepSet, VertexCover, SetCover &#92;) are all NPC. (details of proof are left as an exercise to readers...)


> Now the knowledge is enough for A5. Eric: 101. Anna: 147. Doug: 298.
