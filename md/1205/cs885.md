---
title: CS 885 - Reinforcement Learning
layout: mdtoc
---

# Introduction

## Machine Learning
- Traditional computer science
    - Program computer for every task
- New paradigm
    - Provide examples to machine
    - Machine learns to accomplish a task based on the examples
        - computer vision: feed computer with lots of images, eventually learn to extract correct patterns. Programmers don't need to think of the rules to achieve right things.
        - NLP: learn to do machine translation. Discover patterns to match right expression.
- Success mostly due to supervised learning
    - Bottleneck: need lots of labeled data
- Alternatives
    - Unsupervised learning, semi-supervised learning
    - Reinforcement Learning

## What is RL?
Reinforcement learning is also known as
- Optimal control
- Approximate dynamic programming
- Neuro-dynamic programming

which are from different history or perspective.

From wiki: reinforcement learning is an area of
machine learning inspired by behavioural psychology,
concerned with how software agents ought to take
actions in an environment so as to maximize some
notion of cumulative reward.

It comes from psychology. In animal psychology,
- Negative reinforcements:
    - Pain and hunger
- Positive reinforcements:
    - Pleasure and food
- Reinforcements used to train animals

The problem:

![](/pics/rl_problem.png)


**Goal**: Learn to choose actions that maximize rewards

## Examples
- Game playing (go, atari, backgammon)
- Operations research (pricing, vehicle routing)
- Elevator scheduling
- Helicopter control
- Spoken dialog systems
- Data center energy optimization
- Self-managing network systems
- Autonomous vehicles
- Computational finance

### Operations research
Historically, it was called approximate dynamical programming. Let's look at an exmaple of vehicle routing.
- **Agent**: vehicle routing software
- **Environment**: stochastic *demand*: the orders that come from customers, or needs to ship different parts
- **State**: vehicle location, capacity and depot requests
- **Action**: vehicle route
- **Reward**: - travel costs. We want to minimize the cost.

### Robotic Control
helicopter control. It is very hard to control. Naturally unstable.
- **Agent**: controller
- **Environment**: helicopter, or the air around it.
- **State**: position, orientation, velocity and angular velocity
- **Action**: collective pitch, cyclic pitch, tail rotor control
- **Reward**: - deviation from desired trajectory

2008 (Andrew Ng): automated helicopter wins acrobatic competition against humans. [Quick video](https://youtu.be/0JL04JJjocc)

### Game Playing
Go (one of the oldest and hardest board games)
- **Agent**: player
- **Environment**: opponent
- **State**: board configuration
- **Action**: next stone location
- **Reward**: +1 win / -1 lose
- 2016: AlphaGo defeats top player Lee Sedol (4-1)
    - Game 2 move 37: AlphaGo plays unexpected move (odds 1/10,000)

### Conversational agent
- **Agent**: virtual assistant
- **Environment**: user
- **State**: conversation history
- **Action**: next utterance
- **Reward**: points based on task completion, user satisfaction, etc.
- Today: active area of research

### Computational Finance
Automated trading
- **Agent**: trading software
- **Environment**: other traders
- **State**: price history
- **Action**: buy/sell/hold
- **Reward**: amount of profit

Example: how to purchase a large # of shares in a short period of time without affecting the price

Thus, RL is comprehensive, but challenging form of machine learning
- Stochastic environment
- Incomplete model
- Interdependent sequence of decisions
- No supervision
- Partial and delayed feedback

**Long term goal**: lifelong machine learning

# Markov Processes
If we unroll the problem,
- Unrolling the control loop leads to a sequence of
states, actions and rewards:
&#92;[
    s_ 0, a_ 0, r_ 0, s_ 1, a_ 1, r_ 1, s_ 2, a_ 2, r_ 2, \ldots
&#92;]
(state, action, reward)
- This sequence forms a stochastic process (due to some uncertainty in the dynamics of the process)

## Common Properties
- Processes are rarely arbitrary
- They often exhibit some structure
    - Laws of the process do not change
    - Short history sufficient to predict future

Example: weather prediction.
- Same model can be used everyday to predict weather
- Weather measurements of past few days sufficient to predict weather.

## Stochastic Process
Now consider the sequence of states only

<div class="fancy-block" data-type="Definition" data-title="Stochastic Process">
<div class="fancy-block-content">
A set of states: &#92;(S &#92;). Stochastic dynamics: &#92;(Pr(s_ t&#124;s_ {t-1},\ldots, s_ 0 ) &#92;)
</div></div>
Conditional distribution over the current state given the past states.

![](/pics/sto_pro.png)

However, we might have infinitely large conditional distributions. Solutions:
- Stationary process: dynamics do not change over time
- Markov assumption: current state depends only on a finite history of past states

## K-order Markov Process
- Assumption: last k states sufficient
- First-order Markov Process: &#92;(Pr(s_ t&#124; s_ {t-1},\ldots, s_0)= Pr(s_ t &#124;s_ {t-1}) &#92;)
- Second-order Markov Process: &#92;(Pr(s_ t&#124; s_ {t-1},\ldots, s_0)= Pr(s_ t &#124;s_ {t-1}, s_ {t-2}) &#92;)

By default, a Markov Process refers to a
- First-order process for all &#92;(t &#92;)
- Stationary process: &#92;(Pr(s_ t &#124; s_ {t-1}) = Pr(s_ {t'}&#124; s_ {t'-1}) ~ \forall t'&#92;). This is a bit counterintuitive, but the dynamics is not changing.
- **Advantage**: can specify the entire process with a
single concise conditional distribution &#92;(Pr(s'&#124;s) &#92;).

**Examples**
- Robotic control
    - States: &#92;(\langle x, y, z, \theta \rangle &#92;) coordinates of joints
    - Dynamics: constant motion
- Inventory management
    - States: Inventory level
    - Dynamics: constant (stochastic) demand

## Non-Markovian and/or non-stationary processes
*What if the process is not Markovian and/or not
stationary?*

**Solution**: add new state components until dynamics
are Markovian and stationary
- Robotics: the dynamics of &#92;(\langle x, y, z, \theta \rangle &#92;) are not stationary when velocity varies…
- Solution:  add velocity to state description e.g.  &#92;(\langle x, y, z, \theta, \dot{x}, \dot{y}, \dot{z}, \dot{\theta}\rangle &#92;)
-  If acceleration varies… then add acceleration to state
- Where do we stop?

It's like Taylor series expansion.

**Problem**: adding components to the state
description to force a process to be Markovian and
stationary may significantly increase computational
complexity

**Solution**: try to find the smallest state description
that is self-sufficient (i.e., Markovian and stationary). Not sth ez to come up with.

## Inference in Markov processes
Common task is to do prediction: &#92;(Pr(s_ {t+k}&#124; s_ t) &#92;)

Computation:
&#92;[
    \operatorname{Pr}\left(s_{t+k} &#124; s_{t}\right)=\sum_{s_{t+1} \cdots s_{t+k-1}} \prod_{i=1}^{k} \operatorname{Pr}\left(s_{t+i} &#124; s_{t+i-1}\right)
&#92;]

Discrete states (matrix operations):
- Let &#92;(T &#92;) (transition matrix) be a &#92;(&#124;S&#124;\times &#124;S&#124; &#92;) matrix representing &#92;(Pr(s_ {t+1}&#124;s_ t) &#92;)
- Then &#92;(Pr (s_ {t+k}&#124; s_ t)=T^k &#92;)
- Complexity: &#92;(O(k&#124;S&#124;^3) &#92;)

However,
- Predictions by themselves are useless
- They are only useful when they will influence future decisions
- Hence the ultimate task is *decision making*
- How can we influence the process to visit desirable states?
    - Model: Markov *Decision* Process

# Markov Decision Process
Now let's augment the markov process with actions &#92;(a_ t &#92;) and rewards &#92;( r_ t &#92;).

![](/pics/mdp.png)
