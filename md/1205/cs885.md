---
title: CS 885 - Reinforcement Learning
layout: mdtoc
---

# Introduction

## Machine Learning
- Traditional computer science
    - Program computer for every task
- New paradigm
    - Provide examples to machine
    - Machine learns to accomplish a task based on the examples
        - computer vision: feed computer with lots of images, eventually learn to extract correct patterns. Programmers don't need to think of the rules to achieve right things.
        - NLP: learn to do machine translation. Discover patterns to match right expression.
- Success mostly due to supervised learning
    - Bottleneck: need lots of labeled data
- Alternatives
    - Unsupervised learning, semi-supervised learning
    - Reinforcement Learning

## What is RL?
Reinforcement learning is also known as
- Optimal control
- Approximate dynamic programming
- Neuro-dynamic programming

which are from different history or perspective.

From wiki: reinforcement learning is an area of
machine learning inspired by behavioural psychology,
concerned with how software agents ought to take
actions in an environment so as to maximize some
notion of cumulative reward.

It comes from psychology. In animal psychology,
- Negative reinforcements:
    - Pain and hunger
- Positive reinforcements:
    - Pleasure and food
- Reinforcements used to train animals

The problem:

![](/pics/rl_problem.png)


**Goal**: Learn to choose actions that maximize rewards

## Examples
- Game playing (go, atari, backgammon)
- Operations research (pricing, vehicle routing)
- Elevator scheduling
- Helicopter control
- Spoken dialog systems
- Data center energy optimization
- Self-managing network systems
- Autonomous vehicles
- Computational finance

### Operations research
Historically, it was called approximate dynamical programming. Let's look at an exmaple of vehicle routing.
- **Agent**: vehicle routing software
- **Environment**: stochastic *demand*: the orders that come from customers, or needs to ship different parts
- **State**: vehicle location, capacity and depot requests
- **Action**: vehicle route
- **Reward**: - travel costs. We want to minimize the cost.

### Robotic Control
helicopter control. It is very hard to control. Naturally unstable.
- **Agent**: controller
- **Environment**: helicopter, or the air around it.
- **State**: position, orientation, velocity and angular velocity
- **Action**: collective pitch, cyclic pitch, tail rotor control
- **Reward**: - deviation from desired trajectory

2008 (Andrew Ng): automated helicopter wins acrobatic competition against humans. [Quick video](https://youtu.be/0JL04JJjocc)

### Game Playing
Go (one of the oldest and hardest board games)
- **Agent**: player
- **Environment**: opponent
- **State**: board configuration
- **Action**: next stone location
- **Reward**: +1 win / -1 lose
- 2016: AlphaGo defeats top player Lee Sedol (4-1)
    - Game 2 move 37: AlphaGo plays unexpected move (odds 1/10,000)

### Conversational agent
- **Agent**: virtual assistant
- **Environment**: user
- **State**: conversation history
- **Action**: next utterance
- **Reward**: points based on task completion, user satisfaction, etc.
- Today: active area of research

### Computational Finance
Automated trading
- **Agent**: trading software
- **Environment**: other traders
- **State**: price history
- **Action**: buy/sell/hold
- **Reward**: amount of profit

Example: how to purchase a large # of shares in a short period of time without affecting the price

Thus, RL is comprehensive, but challenging form of machine learning
- Stochastic environment
- Incomplete model
- Interdependent sequence of decisions
- No supervision
- Partial and delayed feedback

**Long term goal**: lifelong machine learning

# Markov Processes
If we unroll the problem,
- Unrolling the control loop leads to a sequence of
states, actions and rewards:
&#92;[
    s_ 0, a_ 0, r_ 0, s_ 1, a_ 1, r_ 1, s_ 2, a_ 2, r_ 2, \ldots
&#92;]
(state, action, reward)
- This sequence forms a stochastic process (due to some uncertainty in the dynamics of the process)

## Common Properties
- Processes are rarely arbitrary
- They often exhibit some structure
    - Laws of the process do not change
    - Short history sufficient to predict future

Example: weather prediction.
- Same model can be used everyday to predict weather
- Weather measurements of past few days sufficient to predict weather.

## Stochastic Process
Now consider the sequence of states only

<div class="fancy-block" data-type="Definition" data-title="Stochastic Process">
<div class="fancy-block-content">
A set of states: &#92;(S &#92;). Stochastic dynamics: &#92;(Pr(s_ t|s_ {t-1},\ldots, s_ 0 ) &#92;)
</div></div>
Conditional distribution over the current state given the past states.
