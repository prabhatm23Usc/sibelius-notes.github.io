---
title: CS 885 - Reinforcement Learning
layout: mdtoc
---

# Introduction

## Machine Learning
- Traditional computer science
    - Program computer for every task
- New paradigm
    - Provide examples to machine
    - Machine learns to accomplish a task based on the examples
        - computer vision: feed computer with lots of images, eventually learn to extract correct patterns. Programmers don't need to think of the rules to achieve right things.
        - NLP: learn to do machine translation. Discover patterns to match right expression.
- Success mostly due to supervised learning
    - Bottleneck: need lots of labeled data
- Alternatives
    - Unsupervised learning, semi-supervised learning
    - Reinforcement Learning

## What is RL?
Reinforcement learning is also known as
- Optimal control
- Approximate dynamic programming
- Neuro-dynamic programming

which are from different history or perspective.

From wiki: reinforcement learning is an area of
machine learning inspired by behavioural psychology,
concerned with how software agents ought to take
actions in an environment so as to maximize some
notion of cumulative reward.

It comes from psychology. In animal psychology,
- Negative reinforcements:
    - Pain and hunger
- Positive reinforcements:
    - Pleasure and food
- Reinforcements used to train animals

The problem:

![](/pics/rl_problem.png)


**Goal**: Learn to choose actions that maximize rewards

## Examples
- Game playing (go, atari, backgammon)
- Operations research (pricing, vehicle routing)
- Elevator scheduling
- Helicopter control
- Spoken dialog systems
- Data center energy optimization
- Self-managing network systems
- Autonomous vehicles
- Computational finance

### Operations research
Historically, it was called approximate dynamical programming. Let's look at an exmaple of vehicle routing.
- **Agent**: vehicle routing software
- **Environment**: stochastic *demand*: the orders that come from customers, or needs to ship different parts
- **State**: vehicle location, capacity and depot requests
- **Action**: vehicle route
- **Reward**: - travel costs. We want to minimize the cost.

### Robotic Control
helicopter control. It is very hard to control. Naturally unstable.
- **Agent**: controller
- **Environment**: helicopter, or the air around it.
- **State**: position, orientation, velocity and angular velocity
- **Action**: collective pitch, cyclic pitch, tail rotor control
- **Reward**: - deviation from desired trajectory

2008 (Andrew Ng): automated helicopter wins acrobatic competition against humans. [Quick video](https://youtu.be/0JL04JJjocc)

### Game Playing
Go (one of the oldest and hardest board games)
- **Agent**: player
- **Environment**: opponent
- **State**: board configuration
- **Action**: next stone location
- **Reward**: +1 win / -1 lose
- 2016: AlphaGo defeats top player Lee Sedol (4-1)
    - Game 2 move 37: AlphaGo plays unexpected move (odds 1/10,000)

### Conversational agent
- **Agent**: virtual assistant
- **Environment**: user
- **State**: conversation history
- **Action**: next utterance
- **Reward**: points based on task completion, user satisfaction, etc.
- Today: active area of research

### Computational Finance
Automated trading
- **Agent**: trading software
- **Environment**: other traders
- **State**: price history
- **Action**: buy/sell/hold
- **Reward**: amount of profit

Example: how to purchase a large # of shares in a short period of time without affecting the price

Thus, RL is comprehensive, but challenging form of machine learning
- Stochastic environment
- Incomplete model
- Interdependent sequence of decisions
- No supervision
- Partial and delayed feedback

**Long term goal**: lifelong machine learning

# Markov Processes
If we unroll the problem,
- Unrolling the control loop leads to a sequence of
states, actions and rewards:
&#92;[
    s_ 0, a_ 0, r_ 0, s_ 1, a_ 1, r_ 1, s_ 2, a_ 2, r_ 2, \ldots
&#92;]
(state, action, reward)
- This sequence forms a stochastic process (due to some uncertainty in the dynamics of the process)

## Common Properties
- Processes are rarely arbitrary
- They often exhibit some structure
    - Laws of the process do not change
    - Short history sufficient to predict future

Example: weather prediction.
- Same model can be used everyday to predict weather
- Weather measurements of past few days sufficient to predict weather.

## Stochastic Process
Now consider the sequence of states only

<div class="fancy-block" data-type="Definition" data-title="Stochastic Process">
<div class="fancy-block-content">
A set of states: &#92;(S &#92;). Stochastic dynamics: &#92;(Pr(s_ t&#124;s_ {t-1},\ldots, s_ 0 ) &#92;)
</div></div>
Conditional distribution over the current state given the past states.

![](/pics/sto_pro.png)

However, we might have infinitely large conditional distributions. Solutions:
- Stationary process: dynamics do not change over time
- Markov assumption: current state depends only on a finite history of past states

## K-order Markov Process
- Assumption: last k states sufficient
- First-order Markov Process: &#92;(Pr(s_ t&#124; s_ {t-1},\ldots, s_0)= Pr(s_ t &#124;s_ {t-1}) &#92;)
- Second-order Markov Process: &#92;(Pr(s_ t&#124; s_ {t-1},\ldots, s_0)= Pr(s_ t &#124;s_ {t-1}, s_ {t-2}) &#92;)

By default, a Markov Process refers to a
- First-order process for all &#92;(t &#92;)
- Stationary process: &#92;(Pr(s_ t &#124; s_ {t-1}) = Pr(s_ {t'}&#124; s_ {t'-1}) ~ \forall t'&#92;). This is a bit counterintuitive, but the dynamics is not changing.
- **Advantage**: can specify the entire process with a
single concise conditional distribution &#92;(Pr(s'&#124;s) &#92;).

**Examples**
- Robotic control
    - States: &#92;(\langle x, y, z, \theta \rangle &#92;) coordinates of joints
    - Dynamics: constant motion
- Inventory management
    - States: Inventory level
    - Dynamics: constant (stochastic) demand

## Non-Markovian and/or non-stationary processes
*What if the process is not Markovian and/or not
stationary?*

**Solution**: add new state components until dynamics
are Markovian and stationary
- Robotics: the dynamics of &#92;(\langle x, y, z, \theta \rangle &#92;) are not stationary when velocity varies…
- Solution:  add velocity to state description e.g.  &#92;(\langle x, y, z, \theta, \dot{x}, \dot{y}, \dot{z}, \dot{\theta}\rangle &#92;)
-  If acceleration varies… then add acceleration to state
- Where do we stop?

It's like Taylor series expansion.

**Problem**: adding components to the state
description to force a process to be Markovian and
stationary may significantly increase computational
complexity

**Solution**: try to find the smallest state description
that is self-sufficient (i.e., Markovian and stationary). Not sth ez to come up with.

## Inference in Markov processes
Common task is to do prediction: &#92;(Pr(s_ {t+k}&#124; s_ t) &#92;)

Computation:
&#92;[
    \operatorname{Pr}\left(s_{t+k} &#124; s_{t}\right)=\sum_{s_{t+1} \cdots s_{t+k-1}} \prod_{i=1}^{k} \operatorname{Pr}\left(s_{t+i} &#124; s_{t+i-1}\right)
&#92;]

Discrete states (matrix operations):
- Let &#92;(T &#92;) (transition matrix) be a &#92;(&#124;S&#124;\times &#124;S&#124; &#92;) matrix representing &#92;(Pr(s_ {t+1}&#124;s_ t) &#92;)
- Then &#92;(Pr (s_ {t+k}&#124; s_ t)=T^k &#92;)
- Complexity: &#92;(O(k&#124;S&#124;^3) &#92;)

However,
- Predictions by themselves are useless
- They are only useful when they will influence future decisions
- Hence the ultimate task is *decision making*
- How can we influence the process to visit desirable states?
    - Model: Markov *Decision* Process

# Markov Decision Process
Now let's augment the markov process with actions &#92;(a_ t &#92;) and rewards &#92;( r_ t &#92;).

![](/pics/mdp.png)

## Current Assumptions
- Uncertainty: *stochastic* process
- Time: *sequential* process
- Observability: *fully* observable states (a bit restrictive for now)
- No learning: *complete* model
- Variable type: *discrete* (e.g., discrete states and actions)

## Rewards
is a real number &#92;(r_t\in\mathfrak R &#92;)

Reward function: &#92;(R(s_ t, a_ t)=r_ t &#92;) mapping from state-action pairs to rewards
- in some situation, it will be pretty clear like in computational finace; in some others like in conversational agents, we need to come up with some numerical signals to capture that property. With this, we will be able to design algorithm to maximize rewards.
- **Common assumption**:  stationary reward function: &#92;(R(s_ t, a_ t) &#92;) is the same &#92;(\forall t &#92;)
- **Exception**: terminal reward function often different
    - E.g., in a game: 0 reward at each turn and +1/-1 at the end for winning/losing
- Goal: &#92;(\max \sum_ t R(s_ t, a_ t) &#92;)

However, if process is infinite, isn't &#92;(\sum_ t R(s_ t, a_ t) &#92;) infinite? Two solutions
- Solution 1: **discounted rewards**
    - Discount factor: &#92;(0\le \gamma < 1 &#92;). Inflation rate... (=1 is fine if we have finite horizon)
    - Finite utility: &#92;(\sum_t \gamma^t R(s_ t,a_ t) &#92;) is a geometric sum
    - &#92;(\gamma &#92;) induces an inflation rate of &#92;(1/\gamma -1 &#92;)
    - Intuition: prefer utility sooner than later
- Solution 2: **average rewards**
    - More complicated computationally
    - Beyond the scope of this course

<div class="fancy-block" data-type="Definition" data-title="Markov Decision Process">
<div class="fancy-block-content">
<ul>
<li>Set of states: &#92;(S &#92;)</li>

<li>Set of actions: &#92;(A &#92;)</li>

<li>Transition model: &#92;(Pr(s_ t &#124; s_ {t-1}, a_ {t-1}) &#92;)</li>

<li>Reward model: &#92;(R(s_ t, a_ t) &#92;)</li>

<li>Discount factor: &#92;(0\le \gamma \le 1 &#92;)


<ul>
<li>discounted: &#92;(\gamma &lt; 1 &#92;)</li>

<li>undiscounted: &#92;(\gamma = 1 &#92;)</li></ul>
</li>

<li>Horizon (i.e., # of time steps): &#92;(h &#92;)


<ul>
<li>Finite horizon: &#92;(h\in\mathbb N &#92;)</li>

<li>Infinite horizon: &#92;(h=\infty &#92;)</li></ul>
</li>
</ul>
</div></div>

**Goal**: find optimal policy

Let's take a look at an example: Inventory management.
-  States: inventory levels
-  Actions: {doNothing, orderWidgets}
-  Transition model: stochastic demand
-  Reward model: Sales - Costs - Storage
-  Discount factor: 0.999
-  Horizon: ∞

*Tradeoff*: increasing supplies decreases odds of missed sales, but increases storage costs

## Policy
is choice of action at each time step. Formally:
- mapping from states to actions
- i.e.m &#92;(\pi(s_ t) = a_ t &#92;)
- Assumptions: fully observable states
    - Allows &#92;(a_t &#92;) to be chosen only based on current state &#92;(s_ t &#92;)

**Policy evaluation**: compute expected utility (expectation)
&#92;[
    V^{\pi}\left(s_{0}\right)=\sum_{t=0}^{h} \gamma^{t} \sum_{s_{t}} \operatorname{Pr}\left(s_{t} | s_{0}, \pi\right) R\left(s_{t}, \pi\left(s_{t}\right)\right)
&#92;]
This expression corresponds to the policy &#92;(\pi &#92;) when we start in state &#92;(s_0 &#92;)

**Optimal policy**: Policy with highest expected utility
&#92;[
    V^{\pi^* } (s_ 0) \ge V^\pi (s_0)\quad \forall \pi
&#92;]
This is the best policy &#92;(\pi^* &#92;).

# Policy Optimization
Several classes of algorithms:
- Value iteration
- Policy iteration
- Linear Programming
- Search techniques

Computation may be done
-  Offline: before the process starts, planning ahead
-  Online: as the process evolves

## Value Iteration
- Performs dynamic programming
- Optimizes decisions in reverse order

Value when no time left:
&#92;[V(s_ h) = \max_ {a_ h} R(s_ h, a_ h) &#92;]

Value with one time step left:
&#92;[
    V\left(s_ {h-1}\right)=\max _ {a_ {h-1}} R\left(s_ {h-1}, a_ {h-1}\right)+\gamma \sum_ {s_ {h}} \operatorname{Pr}\left(s_ {h} &#124; s_ {h-1}, a_ {h-1}\right) V\left(s_ {h}\right)
&#92;]

Value with two time steps left:
&#92;[
    V\left(s_ {h-2}\right)=\max _ {a_ {h-2}} R\left(s_ {h-2}, a_ {h-2}\right)+\gamma \sum_{s_ {h-1}} \operatorname{Pr}\left(s_ {h-1} &#124; s_ {h-2}, a_ {h-2}\right) V\left(s_ {h-1}\right)
&#92;]

...

Bellman's equation:
&#92;[
\begin{aligned}
V\left(s_{t}\right)&=\max _ {a_{t}} R\left(s_ {t}, a_ {t}\right)+\gamma \sum_{s_ {t+1}} \operatorname{Pr}\left(s_{t+1} &#124; s_{t}, a_{t}\right) V\left(s_{t+1}\right) &#92;&#92;
a_{t}^{* }&=\underset{a_ {t}}{\operatorname{argmax}} R\left(s_ {t}, a_ {t}\right)+\gamma \sum_{s_{t+1}} \operatorname{Pr}\left(s_{t+1} &#124; s_{t}, a_{t}\right) V\left(s_{t+1}\right)
\end{aligned}
&#92;]

Finite horizon
-  When h is finite,
-  *Non-stationary* optimal policy
-  Best action different at each time step
-  Intuition: best action varies with the amount of time left

Infinite horizon
-  When h is infinite,
-  *Stationary* optimal policy
-  Same best action at each time step
-  Intuition: same (infinite) amount of time left at each time step, hence same best action
-  **Problem**: value iteration does an infinite number of iterations…

Assuming a discount factor &#92;(\gamma &#92;), after &#92;(n &#92;) time steps, rewards are scaled down by &#92;(\gamma^n &#92;).

For large enough &#92;(n &#92;), rewards become insignificant since &#92;(\gamma^n\to 0 &#92;)

Solution:
- pick large enough &#92;(n &#92;)
- run value iteration for &#92;(n &#92;) steps
- execute policy found at the &#92;(n^{th} &#92;) iteration

### Algorithm
![](/pics/val_inter_mdp.svg)

Optimal policy &#92;(\pi^* &#92;)
- &#92;(t=0 &#92;): &#92;(\pi_0^* (s) \gets \operatorname{argmax}_a R(s,a)\quad \forall s &#92;)
- &#92;(t>0 &#92;): &#92;(\pi_t^* (s)\gets \underset{a}{\operatorname{argmax}} R(s, a)+\gamma \sum_ {s^{\prime}} \operatorname{Pr}\left(s^{\prime} &#124; s, a\right) V_ {t-1}^{* }\left(s^{\prime}\right)\quad  \forall s  &#92;)

**NB**:
- &#92;(t &#92;) indicates the # of time steps to go (till end of process)
- &#92;(\pi^* &#92;) is non stationary (i.e., time dependent)

Matrix form:
- &#92;(R^a &#92;): &#92;(&#124;S &#124;\times 1 &#92;) column vector of rewards for &#92;(a &#92;)
- &#92;(V_t^* &#92;): &#92;(&#124; S &#124;\times 1 &#92;) column vector of state values
- &#92;(T^a &#92;): &#92;(&#124; S &#124; \times &#124; S &#124;&#92;) matrix of transition probability for &#92;(a &#92;)

![](/pics/vi_matrix.svg)

For infinite horizon, let &#92;(h\to \infty &#92;), then &#92;(V_ h^\pi \to V_ \infty^\pi &#92;)

Policy evaluation:
&#92;[
    V_{\infty}^{\pi}(s)=R\left(s, \pi_{\infty}(s)\right)+\gamma \sum_{s^{\prime}} \operatorname{Pr}\left(s^{\prime} &#124; s, \pi_{\infty}(s)\right) V_{\infty}^{\pi}\left(s^{\prime}\right) ~~ \forall s
&#92;]

Bellman's equation:
&#92;[
    V_{\infty}^{* }(s)=\max_ {a} R(s, a)+\gamma \sum_{s^{\prime}} \operatorname{Pr}\left(s^{\prime} &#124; s, a\right) V_{\infty}^{* }\left(s^{\prime}\right)
&#92;]

Let's take a closer look at policy evaluation.
- Linear system of equations
<span>&#92;[
    V_{\infty}^{\pi}(s)=R\left(s, \pi_{\infty}(s)\right)+\gamma \sum_{s^{\prime}} \operatorname{Pr}\left(s^{\prime} | s, \pi_{\infty}(s)\right) V_{\infty}^{\pi}\left(s^{\prime}\right)\quad \forall s
&#92;]</span>
- Matrix form:
    - <span>&#92;(R &#92;)</span>: <span>&#92;(|S|\times 1 &#92;)</span> column vector of state rewards for <span>&#92;(\pi &#92;)</span>
    - <span>&#92;(V &#92;)</span>: <span>&#92;(|S|\times 1 &#92;)</span> column vector of state values for <span>&#92;(\pi &#92;)</span>
    - <span>&#92;(T &#92;)</span>: <span>&#92;(|S|\times |S| &#92;)</span> matrix of transition prob for <span>&#92;(\pi &#92;)</span>

<span>&#92;[
    V=R+\gamma TV
&#92;]</span>

Now we want to solve linear equations. Several ways:
- Gaussian elimination: <span>&#92;((I-\gamma T) V =R&#92;)</span>
- Compute inverse: <span>&#92;(V=(I-\gamma T)^{-1}R &#92;)</span>
- Iterative methods
    - value iteration (a.k.a. Richardson iteration)
    - repeat <span>&#92;(V\gets R+\gamma T V &#92;)</span>

When we do iteration, the idea is to let the value converge. How do we know it converges?

Let <span>&#92;(H(V) \stackrel{\text { def }}{=} R+\gamma T V  &#92;)</span> be the policy eval operator.

<div class="fancy-block"  data-type="Lemma">
<div class="fancy-block-content">
    <span>&#92;(H &#92;)</span> is a contraction mapping.
    <span>&#92;[
        \|H(\tilde V)-H(V)\|_ \infty \le \gamma \| \tilde V - V\|
    &#92;]</span>
</div></div>

**Proof**:
<span>&#92;[
\begin{aligned}
|| H(\tilde{V})-H(V)|| &#95; {\infty}
&=|| R+\gamma T \tilde{V}-R-\gamma T V|| &#95; {\infty} \quad \text { (by definition) } &#92;&#92;
&=|| \gamma T(\tilde{V}-V)|| &#95; {\infty} \quad \text { (simplification) } &#92;&#92;
&\leq \gamma|| T|| &#95; {\infty}|| \tilde{V}-V|| &#95; {\infty} \quad \text { (since }&#92;| A B&#92;| \leq&#92;| A&#92;|&#92;| B&#92;|) &#92;&#92;
&=\gamma|| \tilde{V}-V|| &#95; {\infty} \quad \text { (since } \max &#95; {s} \sum &#95; {s^{\prime}} T\left(s, s^{\prime}\right)=1)
\end{aligned}
&#92;]</span>

<div class="fancy-block"  data-type="Theorem">
<div class="fancy-block-content">
     Policy evaluation converges to <span>&#92;(V^\pi &#92;)</span> for any initial estimate <span>&#92;(V &#92;)</span>
     <span>&#92;[
         \lim_ {n\to\infty} H^{(n)}(V) = V^\pi \quad \forall V
     &#92;]</span>
</div></div>

Proof omitted or see slides...

In practice, we can’t perform an infinite number
of iterations. Suppose that we perform value iteration for <span>&#92;(n &#92;)</span>
steps and <span>&#92;(\|H^{(n)}(V)-H^{(n-1)}(V)\|_ \infty=\epsilon &#92;)</span>, how far is <span>&#92;(H^{(n)}(V) &#92;)</span> from <span>&#92;(V^\pi &#92;)</span>?

<div class="fancy-block"  data-type="Theorem">
<div class="fancy-block-content">
    If <span>&#92;(\|H^{(n)}(V)-H^{(n-1)}(V)\|_ \infty\le\epsilon &#92;)</span> then
    <span>&#92;[
        \|V^\pi - H^{(n)}(V)\|_ \infty \le {\epsilon \over 1-\gamma}
    &#92;]</span>
</div></div>

Proof omitted or see slides...

Now let's take a closer look at *optimal value function*
- Non-linear system of equations
<span>&#92;[
    V_ {\infty}^{* }(s)=\max _ {a} R(s, a)+\gamma \sum _ {s^{\prime}} \operatorname{Pr}\left(s^{\prime} | s, a\right) V _ {\infty}^{* }\left(s^{\prime}\right)\quad \forall s
&#92;]</span>
- Matrix form
    - <span>&#92;(R^a &#92;)</span>: <span>&#92;(|S|\times 1 &#92;)</span> column vector of state rewards for <span>&#92;(a &#92;)</span>
    - <span>&#92;(V^** &#92;)</span>: <span>&#92;(|S|\times 1 &#92;)</span> column vector of state values for optimal values
    - <span>&#92;(T^a &#92;)</span>: <span>&#92;(|S|\times |S| &#92;)</span> matrix of transition prob for <span>&#92;(a &#92;)</span>

<span>&#92;[
    V^* = \max_ a R^a + \gamma T^a V^*
&#92;]</span>

Let <span>&#92;(H^* \stackrel{\text { def }}{=} \max_a R^a \gamma T^a V &#92;)</span> be the operator in value iteration

<div class="fancy-block"  data-type="Lemma">
<div class="fancy-block-content">
    <span>&#92;(H^* &#92;)</span> is a contraction mapping.

    <span>&#92;[
        \|H^* (\tilde V) - H^* (V)\|_ \infty \le \gamma \|\tilde V - V \|_ \infty
    &#92;]</span>
</div></div>

Proof omitted.


## Policy Iteration
